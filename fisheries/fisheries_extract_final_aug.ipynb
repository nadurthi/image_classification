{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisheris using data augmentation\n",
    "\n",
    "## Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "from __future__ import division\n",
    "# # import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import types    \n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import __version__ as keras_version\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU,PReLU\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import average_precision_score,accuracy_score,recall_score,precision_score\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import pdb\n",
    "\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn import linear_model,decomposition\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression,SGDClassifier\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack ,vstack\n",
    "\n",
    "import gc\n",
    "\n",
    "print(gc.collect())\n",
    "\n",
    "import xgboost as xgb\n",
    "from keras import applications\n",
    "import os\n",
    "\n",
    "\n",
    "# deep models\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import multiprocessing as mltproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResNet50.name='ResNet50'\n",
    "VGG16.name='VGG16'\n",
    "VGG19.name='VGG19'\n",
    "InceptionV3.name='InceptionV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Listallimages=[]\n",
    "ALB_path='inputdata/train/ALB'\n",
    "BET_path='inputdata/train/BET'\n",
    "DOL_path='inputdata/train/DOL'\n",
    "LAG_path='inputdata/train/LAG'\n",
    "SHARK_path='inputdata/train/SHARK'\n",
    "YFT_path='inputdata/train/YFT'\n",
    "NoF_path = 'inputdata/train/NoF'\n",
    "OTHER_path = 'inputdata/train/OTHER'\n",
    "\n",
    "\n",
    "images=os.listdir(ALB_path)\n",
    "ALB_images=[os.path.join('inputdata/train/ALB',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "\n",
    "images=os.listdir(BET_path)\n",
    "BET_images=[os.path.join('inputdata/train/BET',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(DOL_path)\n",
    "DOL_images=[os.path.join('inputdata/train/DOL',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(LAG_path)\n",
    "LAG_images=[os.path.join('inputdata/train/LAG',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(SHARK_path)\n",
    "SHARK_images=[os.path.join('inputdata/train/SHARK',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(YFT_path)\n",
    "YFT_images=[os.path.join('inputdata/train/YFT',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(NoF_path)\n",
    "NoF_images=[os.path.join('inputdata/train/NoF',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(OTHER_path)\n",
    "OTHER_images=[os.path.join('inputdata/train/OTHER',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "Allimages={'ALB': ALB_images,\n",
    "           'BET': BET_images,\n",
    "           'DOL': DOL_images,\n",
    "           'LAG': LAG_images,\n",
    "           'SHARK': SHARK_images,\n",
    "           'YFT': YFT_images,\n",
    "           'NoF': NoF_images,\n",
    "           'OTHER': OTHER_images}\n",
    "\n",
    "\n",
    "\n",
    "# Now getting all the test data\n",
    "testimages=[os.path.join('inputdata/test_stg1',ff) for ff in os.listdir('inputdata/test_stg1') if '.jpg' in ff ]+ \\\n",
    "            [os.path.join('inputdata/test_stg2',ff) for ff in os.listdir('inputdata/test_stg2') if '.jpg' in ff ]\n",
    "\n",
    "\n",
    "    \n",
    "# getting cropped images\n",
    "ALB_cropped_path='inputdata/train/ALB/cropped'\n",
    "BET_cropped_path='inputdata/train/BET/cropped'\n",
    "DOL_cropped_path='inputdata/train/DOL/cropped'\n",
    "LAG_cropped_path='inputdata/train/LAG/cropped'\n",
    "SHARK_cropped_path='inputdata/train/SHARK/cropped'\n",
    "YFT_cropped_path='inputdata/train/YFT/cropped'\n",
    "NoF_cropped_path = 'inputdata/train/NoF/cropped'\n",
    "OTHER_cropped_path = 'inputdata/train/OTHER/cropped'\n",
    "\n",
    "images=os.listdir(ALB_cropped_path)\n",
    "ALB_cropped_images=[os.path.join(ALB_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(BET_cropped_path)\n",
    "BET_cropped_images=[os.path.join(BET_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(DOL_cropped_path)\n",
    "DOL_cropped_images=[os.path.join(DOL_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(LAG_cropped_path)\n",
    "LAG_cropped_images=[os.path.join(LAG_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(SHARK_cropped_path)\n",
    "SHARK_cropped_images=[os.path.join(SHARK_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(YFT_cropped_path)\n",
    "YFT_cropped_images=[os.path.join(YFT_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(NoF_cropped_path)\n",
    "NoF_cropped_images=[os.path.join(NoF_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(OTHER_cropped_path)\n",
    "OTHER_cropped_images=[os.path.join(OTHER_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "Allimages_cropped={'ALB': ALB_cropped_images,\n",
    "           'BET': BET_cropped_images,\n",
    "           'DOL': DOL_cropped_images,\n",
    "           'LAG': LAG_cropped_images,\n",
    "           'SHARK': SHARK_cropped_images,\n",
    "           'YFT': YFT_cropped_images,\n",
    "           'NoF': NoF_cropped_images,\n",
    "           'OTHER': OTHER_cropped_images}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ALB_cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the Neural Network models to be run\n",
    "class NNmodels(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 500\n",
    "        self.nb_epoch = 100\n",
    "        self.random_state = 51\n",
    "        self.listofmodels=['NNLinear-1h-500','NNSigmoid-1h-500','NNtanh-1h-500','NNRelu-1h-500','NNLeakyRelu-1h-500']\n",
    "        self.input_dim=None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        i=0\n",
    "        # Single Layer\n",
    "        for l2reg in [0]:\n",
    "            for N1layer in [50,500]:\n",
    "                for dropout in np.arange(0,1,0.2):\n",
    "                    i=i+1\n",
    "#                     if i>5:\n",
    "#                         raise StopIteration\n",
    "                        \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"linear\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNlinear_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'linear'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"sigmoid\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNsigmoid_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'sigmoid'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"tanh\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNtanh_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'tanh'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"relu\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNrelu_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'relu'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(LeakyReLU(alpha = 0.1))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNLeakyReLU_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'LeakyReLU'],model)\n",
    "        \n",
    "        \n",
    "        # Single Layer\n",
    "        for l2reg in np.linspace(0,0.1,5):\n",
    "            for N1layer in [50,500]:\n",
    "                for dropout in [0]:\n",
    "                    i=i+1\n",
    "#                     if i>5:\n",
    "#                         raise StopIteration\n",
    "                        \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"linear\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNlinear_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'linear'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"sigmoid\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNsigmoid_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'sigmoid'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"tanh\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNtanh_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'tanh'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"relu\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNrelu_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'relu'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(LeakyReLU(alpha = 0.1))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NNLeakyReLU_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'LeakyReLU'],model)\n",
    "                    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract feature and train on different models\n",
    "featureExtractors=[\n",
    "                     {'name':'ResNet50','model':ResNet50(weights='imagenet')},\n",
    "                     {'name':'VGG16','model':VGG16(weights='imagenet', include_top=True)},\n",
    "                     {'name':'VGG19','model':VGG19(weights='imagenet', include_top=True)},\n",
    "                     {'name':'InceptionV3','model':InceptionV3(input_tensor=Input(shape=(224, 224, 3)) ,weights='imagenet', include_top=True)}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepFeatureExtractClassify(object):\n",
    "    \n",
    "    def __init__(self,Allimages=[],testimages=[],\n",
    "                     featureExtractors=featureExtractors,\n",
    "                     TrainImageMean='TrainImageMean.npz',\n",
    "                     ModifiedTestData='ModifiedTestData.npz',\n",
    "                     MeanRemovedData='MeanRemovedData.npz',\n",
    "                     ExtractedFeatures='ExtractedFeatures.npz'\n",
    "                        ):\n",
    "        \n",
    "        self.TrainImageMean=TrainImageMean\n",
    "        self.ModifiedTestData=ModifiedTestData\n",
    "        self.MeanRemovedData=MeanRemovedData\n",
    "        self.ExtractedFeatures=ExtractedFeatures\n",
    "        \n",
    "        self.featureExtractors=featureExtractors\n",
    "        self.Allimages=Allimages\n",
    "        self.testimages=testimages\n",
    "        data=np.load('imagenetlabels.npz')\n",
    "        self.ImageNetLabels=data['labels']\n",
    "        \n",
    "        self.NNmodels=NNmodels()\n",
    "        gc.collect()\n",
    "        \n",
    "#     def __del__(self):\n",
    "#         del self.Xmean\n",
    "#         del self.Xext\n",
    "        \n",
    "    def GetImagesMean(self):\n",
    "        if not os.path.isfile(self.TrainImageMean):\n",
    "\n",
    "            mean_R = []\n",
    "            mean_G = []\n",
    "            mean_B = []\n",
    "\n",
    "            #loading images\n",
    "            for imageset in self.Allimages.keys():\n",
    "                print imageset\n",
    "                for img_path in Allimages[imageset]:\n",
    "                    img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "\n",
    "                    #converting images to arrays\n",
    "                    x = image.img_to_array(img)\n",
    "\n",
    "                    #finding the mean image\n",
    "                    a = np.mean(x[:,:,0])\n",
    "                    mean_R = np.append(mean_R,a)\n",
    "                    b = np.mean(x[:,:,1])\n",
    "                    mean_G = np.append(mean_G,b)\n",
    "                    c = np.mean(x[:,:,2])\n",
    "                    mean_B = np.append(mean_B,c)\n",
    "\n",
    "            #Mean Image\n",
    "            self.I_R = np.mean(mean_R)\n",
    "            self.I_G = np.mean(mean_G)\n",
    "            self.I_B = np.mean(mean_B)\n",
    "            print (self.I_R,self.I_G,self.I_B)\n",
    "            np.savez(self.TrainImageMean,I_R=self.I_R,I_G=self.I_G,I_B=self.I_B)\n",
    "        else:\n",
    "            print \"loading saved mean data\"\n",
    "            data=np.load(self.TrainImageMean)\n",
    "            self.I_R=data['I_R']\n",
    "            self.I_G=data['I_G']\n",
    "            self.I_B=data['I_B']\n",
    "            print (self.I_R,self.I_G,self.I_B)\n",
    "    \n",
    "    def SetTestData(self):\n",
    "        if not os.path.isfile(self.ModifiedTestData):\n",
    "            self.testfilenames=None\n",
    "            self.Xtestsubmit=None\n",
    "            for i,img_path in enumerate(self.testimages):\n",
    "                print \"Mean removing for test image \",str(i),\" of \", str( len(self.testimages) ),\"\\r\",\n",
    "                img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "                x = image.img_to_array(img)\n",
    "                x[:,:,0] = x[:,:,0] - self.I_R\n",
    "                x[:,:,1] = x[:,:,1] - self.I_G\n",
    "                x[:,:,2] = x[:,:,2] - self.I_B\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "\n",
    "                if self.Xtestsubmit is None:\n",
    "                    self.Xtestsubmit=x\n",
    "                    self.testfilenames=np.array([img_path])\n",
    "                else:\n",
    "                    self.Xtestsubmit=np.vstack((self.Xtestsubmit,x) )\n",
    "                    self.testfilenames=np.hstack((self.testfilenames,np.array([img_path])) )\n",
    "            np.savez(self.ModifiedTestData,Xtestsubmit=self.Xtestsubmit,testfilenames=self.testfilenames)\n",
    "        else:\n",
    "            print \"loading saved test data\"\n",
    "            data=np.load(self.ModifiedTestData)\n",
    "            self.Xtestsubmit=data['Xtestsubmit']\n",
    "            self.testfilenames=data['testfilenames']\n",
    "            \n",
    "    \n",
    "    def SetMeanRemovedData(self,equalizeclasses=False):\n",
    "        if not os.path.isfile(self.MeanRemovedData):\n",
    "            datagen = ImageDataGenerator(\n",
    "                    rotation_range=90,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "\n",
    "            print \"Removing mean from images\"\n",
    "            Xmean=None\n",
    "            Y=None\n",
    "            for imageset in self.Allimages.keys():\n",
    "                print imageset,' = ',len( self.Allimages[imageset] )\n",
    "                for img_path in self.Allimages[imageset]:\n",
    "                    try:\n",
    "                        img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "                    except:\n",
    "                        print \"error reading image \", img_path\n",
    "                        continue\n",
    "\n",
    "                    x = image.img_to_array(img)\n",
    "                    x[:,:,0] = x[:,:,0] - self.I_R\n",
    "                    x[:,:,1] = x[:,:,1] - self.I_G\n",
    "                    x[:,:,2] = x[:,:,2] - self.I_B\n",
    "\n",
    "                    x = np.expand_dims(x, axis=0)\n",
    "                    if Xmean is None:\n",
    "                        Xmean=x\n",
    "                        Y=np.array([imageset])\n",
    "                    else:\n",
    "                        Xmean=np.vstack((Xmean,x) )\n",
    "                        Y=np.vstack((Y,np.array([imageset])) )\n",
    "\n",
    "            try:\n",
    "                if equalizeclasses==False:\n",
    "                    self.Xmean=Xmean\n",
    "                    self.Y=Y\n",
    "                    i=0\n",
    "                    for batch in datagen.flow(Xmean,Y, batch_size=12):\n",
    "                        i=i+1\n",
    "                        if i>100:\n",
    "                            break\n",
    "                        self.Xmean=np.vstack((self.Xmean,batch[0]) )\n",
    "                        self.Y=np.vstack((self.Y, batch[1] ) )\n",
    "                else:\n",
    "                    self.Xmean=Xmean\n",
    "                    self.Y=Y\n",
    "\n",
    "#                     pdb.set_trace()\n",
    "\n",
    "                    y=Y.reshape(1,-1)[0]\n",
    "                    classsizes=[]\n",
    "                    labels=np.unique(y)\n",
    "                    for cls in labels:\n",
    "                        classsizes.append(len(y[y==cls]))\n",
    "\n",
    "                    classsizes=np.array(classsizes)\n",
    "                    print classsizes\n",
    "                    mxsz=1.3*max(classsizes)\n",
    "                    meansz=1.3*np.mean(classsizes)\n",
    "                    for clsind in range(len(classsizes)):\n",
    "                        print labels[clsind],\" original size \",len(Y[y==labels[clsind]])\n",
    "                        iters= int( (mxsz-classsizes[clsind])/20 ) \n",
    "                        for batch in datagen.flow(Xmean[y==labels[clsind],:,:,:],Y[y==labels[clsind]], batch_size=20):\n",
    "                            self.Xmean=np.vstack((self.Xmean,batch[0]) )\n",
    "                            self.Y=np.vstack((self.Y, batch[1] ) )\n",
    "\n",
    "                            iters=iters-1\n",
    "                            print labels[clsind],clsind,classsizes[clsind],mxsz,iters,'\\r',\n",
    "                            if iters==0:\n",
    "                                break\n",
    "            except:\n",
    "                pdb.set_trace()\n",
    "\n",
    "\n",
    "            # Now try over sample to make the data \n",
    "            print \"original size  = \",Xmean.shape\n",
    "            print \"augmented size  = \",self.Xmean.shape\n",
    "            \n",
    "            self.Labels=np.unique(self.Y)\n",
    "            Y=self.Y\n",
    "            for ind,l in enumerate(self.Labels):\n",
    "                Y[np.argwhere(Y==l)]=ind\n",
    "            self.Ybinary=np_utils.to_categorical(Y)\n",
    "            self.Y=self.Y.astype(int)\n",
    "            self.Ybinary=self.Ybinary.astype(int)\n",
    "            np.savez(self.MeanRemovedData,Xmean=self.Xmean,Y=self.Y,Labels=self.Labels,Ybinary=self.Ybinary)\n",
    "        else:\n",
    "            print \"Loading mean removed data\"\n",
    "            data=np.load(self.MeanRemovedData)\n",
    "            self.Xmean=data['Xmean']\n",
    "            self.Y=data['Y']\n",
    "            self.Y=self.Y.astype(int)\n",
    "\n",
    "            self.Labels=data['Labels']\n",
    "            self.Ybinary=data['Ybinary']  \n",
    "            \n",
    "            \n",
    "            \n",
    "    def ExtractFeatures(self,rerun=False):\n",
    "        if not os.path.isfile(self.ExtractedFeatures) or rerun:\n",
    "#             self.SetMeanRemovedData()\n",
    "            self.Xext={}\n",
    "            for model in featureExtractors:\n",
    "                print \"extracting features using deep covnet \",model['name']\n",
    "                X=None\n",
    "                for i in range( self.Xmean.shape[0] ):\n",
    "                    print \"Working on image ... \"+str(i)+\" of \"+ str(self.Xmean.shape[0]) +\"\\r\",\n",
    "                    x = np.expand_dims(self.Xmean[i], axis=0)\n",
    "                    preds = model['model'].predict(x)\n",
    "                    if X is None:\n",
    "                        X=preds[0]\n",
    "                    else:\n",
    "                        X=np.vstack( (X,preds[0]) )\n",
    "\n",
    "                self.Xext[model['name'] ]=X\n",
    "                print \"\\nDone\\n\"\n",
    "                \n",
    "            np.savez(self.ExtractedFeatures,Xext=self.Xext)\n",
    "        else:\n",
    "            print \"Loading Extracted Features\"\n",
    "            data=np.load(self.ExtractedFeatures)\n",
    "            self.Xext=data['Xext'][()]\n",
    "    \n",
    "    \n",
    "    def ExtractFeatures_parallel(self,rerun=False):\n",
    "        if not os.path.isfile(self.ExtractedFeatures) or rerun:\n",
    "#             self.SetMeanRemovedData()\n",
    "            self.Xext={}\n",
    "            for model in featureExtractors:\n",
    "                print \"extracting features in parallel using deep covnet \",model['name']\n",
    "                X=None\n",
    "                for i in range( self.Xmean.shape[0] ):\n",
    "                    print \"Working on image ... \"+str(i)+\" of \"+ str(self.Xmean.shape[0]) +\"\\r\",\n",
    "                    x = np.expand_dims(self.Xmean[i], axis=0)\n",
    "                    preds = model['model'].predict(x)\n",
    "                    if X is None:\n",
    "                        X=preds[0]\n",
    "                    else:\n",
    "                        X=np.vstack( (X,preds[0]) )\n",
    "\n",
    "                self.Xext[model['name'] ]=X\n",
    "                print \"\\nDone\\n\"\n",
    "                \n",
    "            np.savez(self.ExtractedFeatures,Xext=self.Xext)\n",
    "        else:\n",
    "            print \"Loading Extracted Features\"\n",
    "            data=np.load(self.ExtractedFeatures)\n",
    "            self.Xext=data['Xext'][()]\n",
    "            \n",
    "    def ConvertImage2BBoxes(self,imgarr,boxparas={'size':[(60,30),(30,60)]}):\n",
    "        \"\"\"\n",
    "        (w,h) box parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def GetSplitTrain(self,X,y,uniformizedata=False,N=41):\n",
    "        if not uniformizedata:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=N)\n",
    "            return (X_train, X_test, y_train, y_test)\n",
    "               \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=N)\n",
    "        \n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    def RunNNmodels_parallel(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "        self.NNmodels.input_dim=X_train.shape[1]\n",
    "        \n",
    "        def chunks(l, n):\n",
    "            \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "            for i in range(0, len(l), n):\n",
    "                yield l[i:i + n]\n",
    "\n",
    "        \n",
    "        batches=[]\n",
    "        for name,paras,model in self.NNmodels:\n",
    "            \n",
    "            if name in self.Results[featuremodel['name']].keys():\n",
    "                if 'clfs' in self.Results[featuremodel['name']][name].keys():\n",
    "                    if len(self.Results[featuremodel['name']][name]['clfs'])==1:\n",
    "                        continue\n",
    "            \n",
    "                    \n",
    "            batches.append((name,paras,model) )\n",
    "        \n",
    "        def parallelruns(i,j):\n",
    "            print i,j\n",
    "            for name,paras,model in batches[i:j+1]:\n",
    "                print \"\\n\\n<<<<<<<< ++  >>>>>>>>>>>\"\n",
    "                print \"NN model :: \",name\n",
    "                model.fit(X_train, np_utils.to_categorical(y_train), batch_size=self.NNmodels.batch_size, \n",
    "                          nb_epoch=self.NNmodels.nb_epoch,verbose=0, validation_split=0.3)\n",
    "                score = model.evaluate(X_test,np_utils.to_categorical(y_test), verbose=1)\n",
    "                print \"\"\n",
    "                print '\\n\\n Test score:', score\n",
    "\n",
    "                self.Results[featuremodel['name']][name]={ 'clfs':[model],'paras':[paras] }\n",
    "\n",
    "                dumpname=os.path.join(self.modelspath,  featuremodel['name']+'_'+name+'.h5')\n",
    "                model.save(dumpname)\n",
    "                self.ResultsSave[featuremodel['name']]['XGBOOST']={ 'clfs':[dumpname],'paras':[paras]  }\n",
    "        \n",
    "\n",
    "        P=[]\n",
    "        for chk in chunks(range(len(batches)),100):\n",
    "            P.append( mltproc.Process(target=parallelruns, args=(chk[0],chk[-1],)) )\n",
    "\n",
    "        for p in P:\n",
    "            p.start()\n",
    "        for p in P:\n",
    "            p.join()\n",
    "\n",
    "    def RunNNmodels(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "        self.NNmodels.input_dim=X_train.shape[1]\n",
    "        \n",
    "        i=1\n",
    "        for name,paras,model in self.NNmodels:\n",
    "            if name in self.Results[featuremodel['name']].keys():\n",
    "                if 'clfs' in self.Results[featuremodel['name']][name].keys():\n",
    "                    if len(self.Results[featuremodel['name']][name]['clfs'])==1:\n",
    "                        continue\n",
    "                    \n",
    "            print \"\\n\\n<<<<<<<< ++  >>>>>>>>>>>\"\n",
    "            print \"NN model :: \",name\n",
    "            model.fit(X_train, np_utils.to_categorical(y_train), batch_size=self.NNmodels.batch_size, \n",
    "                      nb_epoch=self.NNmodels.nb_epoch,verbose=0, validation_split=0.3)\n",
    "            score = model.evaluate(X_test,np_utils.to_categorical(y_test), verbose=1)\n",
    "            print \"\"\n",
    "            print '\\n\\n Test score:', score\n",
    "\n",
    "            self.Results[featuremodel['name']][name]={ 'clfs':[model],'paras':[paras] }\n",
    "\n",
    "            dumpname= featuremodel['name']+'_'+name+'.h5'\n",
    "            model.save(os.path.join(self.modelspath,  dumpname))\n",
    "            self.ResultsSave[featuremodel['name']][name]={ 'clfs':[dumpname],'paras':[paras]  }\n",
    "\n",
    "            if i%10==0:\n",
    "                with open(self.Resultsfile,'w') as F:\n",
    "                    json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "            i=i+1\n",
    "            \n",
    "    def RunXGBoost(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "        \n",
    "        if 'XGBOOST' in self.Results[featuremodel['name']].keys():\n",
    "            if 'clfs' in self.Results[featuremodel['name']]['XGBOOST'].keys():\n",
    "                if len(self.Results[featuremodel['name']]['XGBOOST']['clfs'])==1:\n",
    "                    return\n",
    "                    \n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train.reshape(-1,1))\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "        param = {'max_depth':100, 'eta':0.02, 'silent':1, 'objective':'multi:softmax','num_class':8 }\n",
    "        param['nthread'] = 6\n",
    "        param['eval_metric'] = 'mlogloss'\n",
    "        param['subsample'] = 0.7\n",
    "        param['colsample_bytree']= 0.7\n",
    "        param['min_child_weight'] = 0\n",
    "        param['booster'] = \"gblinear\"\n",
    "\n",
    "        watchlist  = [(dtrain,'train')]\n",
    "        num_round = 300\n",
    "        early_stopping_rounds=10\n",
    "\n",
    "        clf_xgb = xgb.train(param, dtrain, num_round, watchlist,early_stopping_rounds=early_stopping_rounds,verbose_eval = False)\n",
    "\n",
    "        Ypred = clf_xgb.predict(dtest)\n",
    "        # y_test=np_utils.to_categorical(y_test)\n",
    "\n",
    "        print \"\\n++ Accuracy Score ++\\n\"\n",
    "        print metrics.accuracy_score(y_test,Ypred)\n",
    "        print \"\\n++ Classification report ++\\n\"\n",
    "        print metrics.classification_report(y_test,Ypred)\n",
    "        print \"\\n++ Confusion Matrix ++\\n\"\n",
    "        print '\\x1b[1;31m'+ str(metrics.confusion_matrix(y_test,Ypred) )+'\\x1b[0m'\n",
    "        \n",
    "        self.Results[featuremodel['name']]['XGBoost']={ 'clfs':[clf_xgb] }\n",
    "        \n",
    "        dumpname= featuremodel['name']+'_'+'XGBOOST.model'\n",
    "        joblib.dump(clf_xgb, os.path.join(self.modelspath, dumpname) )\n",
    "#         clf_xgb.save_model( os.path.join(self.modelspath, dumpname) )\n",
    "        self.ResultsSave[featuremodel['name']]['XGBOOST']={ 'clfs':[dumpname] }\n",
    "    \n",
    "        with open(self.Resultsfile,'w') as F:\n",
    "            json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))            \n",
    "    \n",
    "                    \n",
    "    def RunClassifiers(self,modeldir=None,skipdone=True):\n",
    "        \"\"\"\n",
    "        Run all the classifiers with cross validation and grid search\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if modeldir==None:\n",
    "            self.version=str( datetime.datetime.now()).split('.')[0]\n",
    "            if not os.path.isdir('models_'+self.version):\n",
    "                os.mkdir('models_'+self.version)\n",
    "            self.modelspath='models_'+self.version\n",
    "        else:\n",
    "            self.modelspath=modeldir\n",
    "    \n",
    "        if skipdone==True:\n",
    "            self.LoadModels(modeldir=modeldir)\n",
    "        else:\n",
    "            self.Results={}\n",
    "            self.ResultsSave={}\n",
    "         \n",
    "        self.Resultsfile=os.path.join(self.modelspath, 'Results.json')\n",
    "        \n",
    "        \n",
    "            \n",
    "        for featuremodel in featureExtractors:\n",
    "            print \"running classification on features extracted by \", featuremodel['name']\n",
    "\n",
    "            X_train, X_test, y_train, y_test=self.GetSplitTrain(self.Xext[featuremodel['name'] ],self.Y)\n",
    "\n",
    "            ModelParaGrid=[{'name':'LinearSVC','model':LinearSVC(C=1),'para':{'C': [1, 10, 100, 1000,10000]}},\n",
    "                           {'name':'SVC','model':SVC(C=1.0, kernel='linear',max_iter=1e5,probability=True,shrinking=True),'para':{'C': [1, 10, 100, 1000,10000]} },\n",
    "                            {'name':'LogisticRegression','model':LogisticRegression(C=1e1,n_jobs=5,verbose=1),\n",
    "                             'para':{'C': [1, 10, 100, 1000,10000]}},\n",
    "                            {'name':'RandomForestClassifier','model':RandomForestClassifier(n_estimators=15,\n",
    "                                                                n_jobs=5,max_depth=40,max_features=60),\n",
    "                             'para':{'n_estimators':[10,100,250,500],'max_depth':[10,100,250,500], 'max_features': [30,40,50,60] }},\n",
    "                            ]\n",
    "            if featuremodel['name'] not in self.Results.keys():\n",
    "                self.Results[featuremodel['name']]={}\n",
    "                self.ResultsSave[featuremodel['name']]={}\n",
    "            \n",
    "            print \"#####################################################################################\"\n",
    "            print \"--------------------  \"+ featuremodel['name'] +\"  -----------------------------------\"\n",
    "            print \"#####################################################################################\"\n",
    "\n",
    "\n",
    "            for M in ModelParaGrid:\n",
    "                scores = ['precision', 'recall']\n",
    "                print \"************ \" + M['name'] + \"******************\"\n",
    "                \n",
    "                if M['name'] not in self.Results[featuremodel['name']].keys():\n",
    "                    self.Results[featuremodel['name']][M['name']]={'clfs':[] }\n",
    "                    self.ResultsSave[featuremodel['name']][M['name']]={'clfs':[]}\n",
    "\n",
    "                if len( self.Results[featuremodel['name']][M['name']]['clfs'] )==0:\n",
    "                    for score in scores:\n",
    "                        print \"# Tuning hyper-parameters for %s\" % score\n",
    "                        print \"\"\n",
    "\n",
    "                        clf = GridSearchCV(M['model'], M['para'], cv=3,\n",
    "                                           scoring='%s_macro' % score,n_jobs=5)\n",
    "                        clf.fit(X_train,  y_train.reshape(1,-1)[0])\n",
    "\n",
    "                        print \"Best parameters set found on development set:\"\n",
    "                        print \n",
    "                        print clf.best_params_\n",
    "                        print \n",
    "                        print \"Grid scores on development set:\"\n",
    "                        print\n",
    "                        means = clf.cv_results_['mean_test_score']\n",
    "                        stds = clf.cv_results_['std_test_score']\n",
    "                        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "                            print \"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params) \n",
    "                        print\n",
    "\n",
    "                        print \"Detailed classification report:\"\n",
    "                        print\n",
    "                        print \"The model is trained on the full development set.\"\n",
    "                        print \"The scores are computed on the full evaluation set.\"\n",
    "                        print\n",
    "                        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "                        print classification_report(y_true, y_pred)\n",
    "\n",
    "                        print\n",
    "                        self.Results[featuremodel['name']][M['name']]['clfs'].append( clf )\n",
    "\n",
    "                        dumpname= featuremodel['name']+'_'+M['name']+'_'+score+'.pkl'\n",
    "                        joblib.dump(clf, os.path.join(self.modelspath, dumpname) )\n",
    "                        self.ResultsSave[featuremodel['name']][M['name']]['clfs'].append(dumpname)\n",
    "                        \n",
    "                        with open(self.Resultsfile,'w') as F:\n",
    "                            json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "                \n",
    "            # XGBoost\n",
    "            print '-----------------  XGBOOST - tree  ------------------------------'\n",
    "            self.RunXGBoost(featuremodel,X_train, X_test, y_train, y_test)\n",
    "\n",
    "            # Running keras models on extracted features\n",
    "#             print \"----------------- Keras NN models ----------------------------------\"\n",
    "#             self.RunNNmodels(featuremodel,X_train, X_test, y_train, y_test)\n",
    "#             self.RunNNmodels_parallel(featuremodel,X_train, X_test, y_train, y_test)\n",
    "            \n",
    "        \n",
    "            with open(self.Resultsfile,'w') as F:\n",
    "                json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "    \n",
    "    def LoadModelfromfile(self,modeldir,ff):\n",
    "        if '.pkl' in ff:\n",
    "            clf = joblib.load(os.path.join(modeldir,ff) )\n",
    "        elif '.model' in ff:\n",
    "            clf = joblib.load(os.path.join(modeldir,ff))\n",
    "            def predict_custom(self,X):\n",
    "                dtest = xgb.DMatrix(X)\n",
    "                y=self.predict(dtest)\n",
    "                return np_utils.to_categorical(y.reshape(-1,1))\n",
    "            clf.predict_custom=types.MethodType( predict_custom, clf )\n",
    "\n",
    "        elif '.h5' in ff:\n",
    "            clf = load_model(os.path.join(modeldir,ff))\n",
    "        else:\n",
    "            print \"Model name not knwon\"\n",
    "            clf=None\n",
    "        return clf\n",
    "    \n",
    "    \n",
    "    def LoadModels(self,modeldir):\n",
    "        if modeldir is None:\n",
    "#             modeldirectories=[ff for ff in os.listdir('.') if os.path.isdir(ff) and 'model' in ff]\n",
    "            raise \"error error modeldir\"\n",
    "    \n",
    "        print modeldir\n",
    "        self.Resultsfile=os.path.join( modeldir , 'Results.json')\n",
    "        if os.path.isfile(self.Resultsfile):\n",
    "            with open(self.Resultsfile,'r') as F:\n",
    "                self.ResultsSave=json.load(F)\n",
    "            \n",
    "            print \"loading saved models from the saved json\"\n",
    "            \n",
    "            self.Results=copy.deepcopy(self.ResultsSave)\n",
    "            print \"Lazy load ... model is loaded when predcition needed\"\n",
    "            return \n",
    "        \n",
    "            if not hasattr(self,'Results'):\n",
    "                self.Results={}\n",
    "    \n",
    "            for FM in self.ResultsSave.keys():\n",
    "                if FM not in self.Results.keys():\n",
    "                    self.Results[FM]={}\n",
    "\n",
    "                for clftype in self.ResultsSave[FM].keys():\n",
    "                    if clftype not in self.Results[FM].keys():\n",
    "                        self.Results[FM][clftype]={'clfs':[]}\n",
    "                    if len( self.Results[FM][clftype]['clfs'] )==0:\n",
    "                        print \"Loading model for \",FM,\" \",clftype,'\\r',\n",
    "                        \n",
    "                        for i in range( len(self.ResultsSave[FM][clftype]['clfs']) ):\n",
    "                            ff=self.ResultsSave[FM][clftype]['clfs'][i]\n",
    "                            if '.pkl' in ff:\n",
    "                                clf = joblib.load(os.path.join(modeldir,ff) )\n",
    "                            elif '.model' in ff:\n",
    "    #                             clf = xgb.Booster({'nthread':4}) #init model\n",
    "    #                             clf.load_model(os.path.join(modeldir,ff)) # load data\n",
    "                                clf = joblib.load(os.path.join(modeldir,ff))\n",
    "                                def predict_custom(self,X):\n",
    "                                    dtest = xgb.DMatrix(X)\n",
    "                                    y=self.predict(dtest)\n",
    "                                    return np_utils.to_categorical(y.reshape(-1,1))\n",
    "                                clf.predict_custom=types.MethodType( predict_custom, clf )\n",
    "\n",
    "                            elif '.h5' in ff:\n",
    "                                clf = load_model(os.path.join(modeldir,ff))\n",
    "                            else:\n",
    "                                print \"Model name not knwon\"\n",
    "                                clf=None\n",
    "                            self.Results[FM][clftype]['clfs'].append(clf)\n",
    "        else:\n",
    "            self.Results={}\n",
    "            self.ResultsSave={}\n",
    "               \n",
    "            # construct from the available\n",
    "            print \"Results file not there\"\n",
    "            i=0\n",
    "            for model in os.listdir(modeldir):\n",
    "                FE=model.split('_')[0]\n",
    "                clfstr=\".\".join( \"_\".join(model.split('_')[1:]).split('.')[:-1] )\n",
    "                clfstr=clfstr.replace('_recall','').replace('_precision','')\n",
    "                i=i+1\n",
    "                print \"Loading model for \",model,\" \",i,\" of \",len(os.listdir(modeldir)),'\\r',\n",
    "                \n",
    "                if FE not in self.Results.keys():\n",
    "                    self.Results[FE]={}\n",
    "                    self.ResultsSave[FE]={}\n",
    "                if clfstr not in self.Results[FE].keys():\n",
    "                    self.Results[FE][clfstr]={'clfs':[]}\n",
    "                    self.ResultsSave[FE][clfstr]={'clfs':[]}\n",
    "\n",
    "                if '.pkl' in model:\n",
    "                    clf = joblib.load(os.path.join(modeldir,model))\n",
    "                elif '.model' in model:\n",
    "#                     clf = xgb.Booster({'nthread':4}) #init model\n",
    "#                     clf.load_model(os.path.join(modeldir,model)) # load data\n",
    "                    clf = joblib.load(os.path.join(modeldir,model))\n",
    "                    def predict_custom(self,X):\n",
    "                        dtest = xgb.DMatrix(X)\n",
    "                        y=self.predict(dtest)\n",
    "                        return np_utils.to_categorical(y.reshape(-1,1))\n",
    "                    clf.predict_custom=types.MethodType( predict_custom, clf )\n",
    "                    \n",
    "                elif '.h5' in model:\n",
    "#                     print os.path.join(modeldir,model)\n",
    "                    clf = load_model(os.path.join(modeldir,model))\n",
    "                else:\n",
    "                    print \"Model name not knwon\"\n",
    "                    clf=None\n",
    "                self.Results[FE][clfstr]['clfs'].append(clf)\n",
    "                self.ResultsSave[FE][clfstr]['clfs'].append(model)\n",
    "                \n",
    "            with open(self.Resultsfile,'w') as F:\n",
    "                json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "    \n",
    "    def doprediction(self,clf,Xtest):\n",
    "        \n",
    "        if hasattr(clf,'predict_custom'):\n",
    "            y_pred=clf.predict_custom(Xtest)\n",
    "        elif hasattr(clf,'predict_proba'):\n",
    "            y_pred=clf.predict_proba(Xtest)\n",
    "        else:\n",
    "            y_pred=clf.predict(Xtest)\n",
    "            try:\n",
    "                y_pred.shape[1]\n",
    "            except:\n",
    "                y_pred=np_utils.to_categorical(y_pred.reshape(-1,1))\n",
    "\n",
    "        return y_pred\n",
    "        \n",
    "    def EvalPerformance(self,modeldir='',savetag=''):\n",
    "        self.PerFormance=[]\n",
    "        for N in range(25):\n",
    "\n",
    "            print \"Generating random test data \",N\n",
    "            train_idx, test_idx, _, _ = train_test_split(np.arange(self.Xmean.shape[0]), np.arange(self.Xmean.shape[0]), test_size=0.33, random_state=41+N)\n",
    "\n",
    "            print \"Evaluating performance\"\n",
    "            \n",
    "            for FM in self.Results.keys():\n",
    "                print \"--------- \",FM, \" -----------\"\n",
    "\n",
    "                Xtest=self.Xext[ FM ][test_idx,:]\n",
    "                Ytest=self.Y[test_idx]\n",
    "\n",
    "                for clftype in self.Results[FM].keys():\n",
    "                    if 'NN' in clftype:\n",
    "                        continue\n",
    "                    for i in range( len(self.Results[FM][clftype]['clfs']) ):\n",
    "                        print \"\\n\\n\"+FM+' '+clftype+' '+str(i)\n",
    "\n",
    "                        clf=self.Results[FM][clftype]['clfs'][i]\n",
    "                        if isinstance(clf, str) or isinstance(clf, unicode):\n",
    "                            clf=self.LoadModelfromfile(modeldir,clf)\n",
    "                            self.Results[FM][clftype]['clfs'][i]=clf\n",
    "                            \n",
    "                        if hasattr(clf,'predict_custom'):\n",
    "                            y_pred=clf.predict_custom(Xtest)\n",
    "                        elif hasattr(clf,'predict_proba'):\n",
    "                            y_pred=clf.predict_proba(Xtest)\n",
    "                        else:\n",
    "                            y_pred=clf.predict(Xtest)\n",
    "                            try:\n",
    "                                y_pred.shape[1]\n",
    "                            except:\n",
    "                                y_pred=np_utils.to_categorical(y_pred.reshape(-1,1))\n",
    "\n",
    "                        logloss=log_loss(Ytest, y_pred, eps=1e-15, normalize=True)\n",
    "                        avgprec= average_precision_score(np_utils.to_categorical(Ytest), y_pred)\n",
    "                        acc= accuracy_score(Ytest, np.argmax(y_pred,axis=1) )\n",
    "                        recallscore= recall_score(Ytest, np.argmax(y_pred,axis=1),average='micro' )\n",
    "                        precisionscore= precision_score(Ytest, np.argmax(y_pred,axis=1),average='micro' )\n",
    "                        \n",
    "                        classifierfamily=clftype\n",
    "                        if '_NN' in classifierfamily:\n",
    "                            classifierfamily='NN'\n",
    "                        elif 'XGBOOST' in classifierfamily:\n",
    "                            classifierfamily='XGBOOST'\n",
    "                        elif 'LinearSVC' in classifierfamily:\n",
    "                            classifierfamily='LinearSVC'\n",
    "                        elif 'SVC' in classifierfamily:\n",
    "                            classifierfamily='SVC'\n",
    "                        elif 'RandomForestClassifier' in classifierfamily:\n",
    "                            classifierfamily='RandomForestClassifier'\n",
    "                        elif 'LogisticRegression' in classifierfamily:\n",
    "                            classifierfamily='LogisticRegression'\n",
    "                        else:\n",
    "                            classifierfamily=None\n",
    "                        \n",
    "                            \n",
    "                        self.PerFormance.append( {'FeatureExtractor': FM, \n",
    "                                                  'Classifier':clftype+'_'+str(i), \n",
    "                                                  'classifierfamily':classifierfamily,\n",
    "                                                  'log_loss' : logloss,\n",
    "                                                  'acc'   : acc,\n",
    "                                                  'avgprec':avgprec,\n",
    "                                                  'recallscore':recallscore,\n",
    "                                                  'precisionscore':precisionscore,\n",
    "                                                 } )\n",
    "                        print \"\\r\",\n",
    "\n",
    "        df=pd.DataFrame(self.PerFormance)\n",
    "        df.sort_values(by='log_loss',ascending=True,inplace=True)\n",
    "\n",
    "        print df[['FeatureExtractor','Classifier','log_loss','acc']]\n",
    "\n",
    "        df.to_csv('Performance_'+savetag+'.csv')\n",
    "        df.to_hdf('Performance_'+savetag+'.h5','table')\n",
    "\n",
    "\n",
    "    def GenerateSubmission(self,top=10,modeldir=None,savetag=''):\n",
    "        self.LoadModels(modeldir=modeldir)\n",
    "\n",
    "        df=pd.read_hdf('Performance_'+savetag+'.h5','table')\n",
    "\n",
    "        df.sort_values(by='log_loss',ascending=True,inplace=True)\n",
    "        df=df[['FeatureExtractor','Classifier','log_loss','acc']].groupby(['FeatureExtractor','Classifier']).agg('mean').sort_values(by='log_loss',ascending=True)\n",
    "        df=df.reset_index()\n",
    "\n",
    "        for ind in df.index[0:top]:\n",
    "            print df.loc[ind,:]\n",
    "            FM=df.loc[ind,'FeatureExtractor']\n",
    "            Classifier=df.loc[ind,'Classifier']\n",
    "            clftype=\"_\".join( Classifier.split('_')[:-1] ) \n",
    "            Classifier_index=int( Classifier.split('_')[-1] )\n",
    "\n",
    "            clf=self.ResultsSave[ FM ][clftype]['clfs'][Classifier_index]\n",
    "            if isinstance(clf, str) or isinstance(clf, unicode):\n",
    "                clf=self.LoadModelfromfile(modeldir,clf)\n",
    "                self.ResultsSave[ FM ][clftype]['clfs'][Classifier_index]=clf\n",
    "\n",
    "            # first extract features using CNN\n",
    "            testextdata = 'Extractedtestdata_'+FM+'.npz'\n",
    "            Xext=None\n",
    "            if not os.path.isfile(testextdata):\n",
    "                data=np.load(self.ModifiedTestData)\n",
    "                Xtestsubmit=data['Xtestsubmit']\n",
    "                testfilenames=data['testfilenames']\n",
    "\n",
    "                for model in featureExtractors:\n",
    "                    if model['name']!=FM:\n",
    "                        continue\n",
    "                    print \"extracting features using deep covnet \",model['name']\n",
    "                    X=None\n",
    "                    for i in range( Xtestsubmit.shape[0] ):\n",
    "                        print \"Working on image ... \"+str(i)+\" of \"+ str(Xtestsubmit.shape[0]) +\"\\r\",\n",
    "                        x = np.expand_dims(Xtestsubmit[i], axis=0)\n",
    "                        preds = model['model'].predict(x)\n",
    "                        if Xext is None:\n",
    "                            Xext=preds[0]\n",
    "                        else:\n",
    "                            Xext=np.vstack( (Xext,preds[0]) )\n",
    "\n",
    "                    print \"\\nDone and saving\\n\"\n",
    "\n",
    "                np.savez(testextdata,Xext=Xext)\n",
    "                del Xtestsubmit\n",
    "\n",
    "            else:\n",
    "                print \"loading saved data\"\n",
    "                data=np.load(testextdata)\n",
    "                Xext=data['Xext']\n",
    "\n",
    "                data=np.load(self.ModifiedTestData)\n",
    "                testfilenames=data['testfilenames']\n",
    "\n",
    "            print \"doing prediction\"\n",
    "            y_pred=self.doprediction(clf,Xext)\n",
    "            dd=pd.DataFrame({'image':testfilenames, 'ALB': y_pred[:,0], 'BET': y_pred[:,1], 'DOL': y_pred[:,2], \n",
    "                             'LAG': y_pred[:,3], 'NoF': y_pred[:,4], 'OTHER': y_pred[:,5], 'SHARK': y_pred[:,6], 'YFT': y_pred[:,7]})\n",
    "\n",
    "            dd['image']=dd['image'].str.replace('inputdata/','')\n",
    "            dd['image']=dd['image'].str.replace('test_stg1/','')\n",
    "            dd.sort_values(by='image',inplace=True)\n",
    "            dd[['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT']].to_csv('submission_'+FM+'_'+Classifier+'.csv',header=True,index=False)\n",
    "\n",
    "\n",
    "            del Xext\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Run on augmented data\n",
    "# DFEC=DeepFeatureExtractClassify(Allimages=Allimages,testimages=testimages,\n",
    "#                                         TrainImageMean='TrainImageMean.npz',\n",
    "#                                         ModifiedTestData='MeanRemovedTestData.npz',\n",
    "#                                         MeanRemovedData='MeanRemovedData_aug.npz',\n",
    "#                                         ExtractedFeatures='ExtractedFeatures_aug.npz')\n",
    "# DFEC.GetImagesMean()\n",
    "# # #DFEC.SetTestData()\n",
    "# DFEC.SetMeanRemovedData()\n",
    "# DFEC.ExtractFeatures(rerun=False)\n",
    "# DFEC.RunClassifiers(modeldir='models_2017-04-20 14:34:12',skipdone=True)\n",
    "# DFEC.EvalPerformance(modeldir='models_2017-04-20 14:34:12',savetag='aug')\n",
    "# DFEC.GenerateSubmission(top=10,modeldir='models_2017-04-20 14:34:12',savetag='aug')\n",
    "# df=pd.read_hdf('Performance_aug.h5','table')\n",
    "# ds=df[['FeatureExtractor','Classifier','log_loss','acc']].groupby(['FeatureExtractor','Classifier']).agg('mean').sort_values(by='log_loss',ascending=True).reset_index()\n",
    "\n",
    "# DFEC.GenerateSubmission(top=10,modeldir='models_2017-04-20 14:34:12',savetag='aug')\n",
    "# df=pd.read_hdf('Performance_aug.h5','table')\n",
    "# ds=df[['FeatureExtractor','Classifier','log_loss','acc']].groupby(['FeatureExtractor','Classifier']).agg('mean').sort_values(by='log_loss',ascending=True).reset_index()\n",
    "# ds.to_html('best-classifiers_aug.html')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig,ax=plt.subplots(1,1,figsize=(10,30) )\n",
    "# df.boxplot(column=['log_loss'], by=['FeatureExtractor','Classifier'],vert=False,ax=ax)\n",
    "# plt.savefig('box_aug_loss.png',bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# fig,ax=plt.subplots(1,1,figsize=(10,30) )\n",
    "# df.boxplot(column=['acc'], by=['FeatureExtractor','Classifier'],vert=False,ax=ax)\n",
    "# plt.savefig('box_aug_acc.png',bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Run un-augmented data\n",
    "# DFEC=DeepFeatureExtractClassify(Allimages=Allimages,testimages=testimages,\n",
    "#                                         TrainImageMean='TrainImageMean.npz',\n",
    "#                                         ModifiedTestData='MeanRemovedTestData.npz',\n",
    "#                                         MeanRemovedData='MeanRemovedData_noaug.npz',\n",
    "#                                         ExtractedFeatures='ExtractedFeatures_noaug.npz')\n",
    "\n",
    "# DFEC.GetImagesMean()\n",
    "# DFEC.SetMeanRemovedData()\n",
    "# DFEC.ExtractFeatures(rerun=False)\n",
    "# DFEC.RunClassifiers(modeldir='models_noaug',skipdone=True)\n",
    "# DFEC.EvalPerformance(modeldir='models_noaug',savetag='noaug')\n",
    "# DFEC.GenerateSubmission(top=10,modeldir='models_noaug',savetag='noaug')\n",
    "# df=pd.read_hdf('Performance_aug.h5','table')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved mean data\n",
      "(array(96.40368659262118), array(107.12191881841852), array(99.89469193894617))\n",
      "Removing mean from images\n",
      "YFT  =  539\n",
      "error reading image  inputdata/train/YFT/cropped/cropped_0_img_04097.jpg\n",
      "error reading image  inputdata/train/YFT/cropped/cropped_1_img_04097.jpg\n",
      "SHARK  =  148\n",
      "OTHER  =  273\n",
      "error reading image  inputdata/train/OTHER/cropped/cropped_1_img_06708.jpg\n",
      "DOL  =  110\n",
      "error reading image  inputdata/train/DOL/cropped/cropped_0_img_00983.jpg\n",
      "error reading image  inputdata/train/DOL/cropped/cropped_0_img_07643.jpg\n",
      "error reading image  inputdata/train/DOL/cropped/cropped_1_img_06204.jpg\n",
      "error reading image  inputdata/train/DOL/cropped/cropped_0_img_06204.jpg\n",
      "error reading image  inputdata/train/DOL/cropped/cropped_2_img_06204.jpg\n",
      "NoF  =  334\n",
      "error reading image  inputdata/train/NoF/cropped/cropped_5_img_01421.jpg\n",
      "LAG  =  72\n",
      "ALB  =  727\n",
      "BET  =  254\n",
      "> <ipython-input-32-f75415e86dae>(151)SetMeanRemovedData()\n",
      "-> y=Y.reshape(1,-1)[0]\n",
      "(Pdb) c\n",
      "[727 254 105  72 333 272 148 537]\n",
      "ALB  original size  727\n",
      "BET  original size  254\n",
      "DOL  original size  105\n",
      "LAG  original size  72\n",
      "NoF  original size  333\n",
      "OTHER  original size  272\n",
      "SHARK  original size  148\n",
      "YFT  original size  537\n",
      "original size  =  (2448, 224, 224, 3)\n",
      "augmented size  =  (7200, 224, 224, 3)\n",
      "extracting features using deep covnet  ResNet50\n",
      "Working on image ... 3205 of 7200\r"
     ]
    }
   ],
   "source": [
    "# Run cropped - augmented data\n",
    "DFEC=DeepFeatureExtractClassify(Allimages=Allimages_cropped,testimages=testimages,\n",
    "                                        TrainImageMean='TrainImageMean_cropped.npz',\n",
    "                                        ModifiedTestData='MeanRemovedTestData.npz',\n",
    "                                        MeanRemovedData='MeanRemovedData_cropped.npz',\n",
    "                                        ExtractedFeatures='ExtractedFeatures_cropped.npz')\n",
    "\n",
    "DFEC.GetImagesMean()\n",
    "DFEC.SetMeanRemovedData(equalizeclasses=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DFEC.ExtractFeatures(rerun=False)\n",
    "DFEC.RunClassifiers(modeldir='models_cropped',skipdone=True)\n",
    "DFEC.EvalPerformance(modeldir='models_cropped',savetag='cropped')\n",
    "# DFEC.GenerateSubmission(top=10,modeldir='models_cropped',savetag='cropped')\n",
    "# df=pd.read_hdf('Performance_cropped.h5','table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile('inputdata/train/YFT/cropped/cropped_0_img_04097.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path=Allimages['ALB'][0]\n",
    "\n",
    "plt.figure()\n",
    "img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "#converting images to arrays\n",
    "x = image.img_to_array(img)\n",
    "print type(x),x.shape\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x has first height and then width\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "\n",
    "clf=DFEC.ResultsSave['VGG16']['RandomForestClassifier']['clfs'][0]\n",
    "if isinstance(clf, str) or isinstance(clf, unicode):\n",
    "    clf=DFEC.LoadModelfromfile('models_cropped',clf)\n",
    "    DFEC.Results['VGG16']['RandomForestClassifier']['clfs'][0]=clf\n",
    "\n",
    "\n",
    "for model in featureExtractors:\n",
    "    if model['name']=='VGG16':\n",
    "        break\n",
    "\n",
    "\n",
    "height=100\n",
    "width=50\n",
    "for j in range(0,224,10):\n",
    "    for i in range(0,224,10):\n",
    "        x0=i\n",
    "        y0=j\n",
    "        x1=x0+width\n",
    "        y1=y0+height\n",
    "        \n",
    "        if x1>=224 or y1>=224:\n",
    "            break\n",
    "            \n",
    "        img_path=Allimages['ALB'][0]\n",
    "\n",
    "        fig,ax=plt.subplots(1,2)\n",
    "        img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "        draw=ImageDraw.Draw(img)\n",
    "        draw.rectangle([(x0,y0),(x1,y1)],outline='#ff0000')\n",
    "        del draw\n",
    "        ax[0].imshow(img)\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "        x = image.img_to_array(img)\n",
    "        immg=image.array_to_img(x[y0:y1,x0:x1,:])\n",
    "        immg=immg.resize((224, 224))\n",
    "        y = image.img_to_array(immg)\n",
    "        y = np.expand_dims(y, axis=0)\n",
    "        ax[1].imshow(immg)\n",
    "        \n",
    "        yext=model['model'].predict(y)\n",
    "        fig.suptitle(str( clf.predict_proba(yext)) )\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        print \"-\"*100\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DFEC.Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "img = cv2.imread(img_path,1)\n",
    "img = cv2.resize(img, (224, 224), cv2.INTER_LINEAR).astype(np.uint8)\n",
    "# img = img.transpose((1,0,2))\n",
    "# im = np.expand_dims(im, axis=0)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print type(img),img.shape\n",
    "plt.imshow(img[:100,:100,:])\n",
    "# plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "# //w=187,h=76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=Y.reshape(1,-1)[0]\n",
    "Xmean[y==0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load('MeanRemovedData_cropped.npz')\n",
    "Xmean=data['Xmean']\n",
    "Y=data['Y']\n",
    "Y=Y.astype(int)\n",
    "\n",
    "Labels=data['Labels']\n",
    "Ybinary=data['Ybinary']\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=90,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "y=Y.reshape(1,-1)[0]\n",
    "classsizes=[]\n",
    "for i in range(0,len(np.unique(y))):\n",
    "    classsizes.append(len(y[y==i]))\n",
    "\n",
    "classsizes=np.array(classsizes)\n",
    "mxsz=1.3*max(classsizes)\n",
    "meansz=1.3*np.mean(classsizes)\n",
    "for clsind in range(len(classsizes)):\n",
    "    print \n",
    "    iters= int( (mxsz-classsizes[clsind])/20 ) \n",
    "    for batch in datagen.flow(Xmean[y==clsind,:,:,:],Y[y==clsind], batch_size=20):\n",
    "        Xmean=np.vstack((Xmean,batch[0]) )\n",
    "        Y=np.vstack((Y, batch[1] ) )\n",
    "        \n",
    "        iters=iters-1\n",
    "        print clsind,classsizes[clsind],mxsz,iters,'\\r',\n",
    "        if iters==0:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xmean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
