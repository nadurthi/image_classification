{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "from __future__ import division\n",
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import __version__ as keras_version\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU,PReLU\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import pdb\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn import linear_model,decomposition\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression,SGDClassifier\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack ,vstack\n",
    "\n",
    "import gc\n",
    "\n",
    "print(gc.collect())\n",
    "\n",
    "import xgboost as xgb\n",
    "from keras import applications\n",
    "import os\n",
    "\n",
    "\n",
    "# deep models\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResNet50.name='ResNet50'\n",
    "VGG16.name='VGG16'\n",
    "VGG19.name='VGG19'\n",
    "InceptionV3.name='InceptionV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Listallimages=[]\n",
    "ALB_path='inputdata/train/ALB'\n",
    "BET_path='inputdata/train/BET'\n",
    "DOL_path='inputdata/train/DOL'\n",
    "LAG_path='inputdata/train/LAG'\n",
    "SHARK_path='inputdata/train/SHARK'\n",
    "YFT_path='inputdata/train/YFT'\n",
    "NoF_path = 'inputdata/train/NoF'\n",
    "OTHER_path = 'inputdata/train/OTHER'\n",
    "\n",
    "\n",
    "images=os.listdir(ALB_path)\n",
    "ALB_images=[os.path.join('inputdata/train/ALB',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "\n",
    "images=os.listdir(BET_path)\n",
    "BET_images=[os.path.join('inputdata/train/BET',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(DOL_path)\n",
    "DOL_images=[os.path.join('inputdata/train/DOL',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(LAG_path)\n",
    "LAG_images=[os.path.join('inputdata/train/LAG',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(SHARK_path)\n",
    "SHARK_images=[os.path.join('inputdata/train/SHARK',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(YFT_path)\n",
    "YFT_images=[os.path.join('inputdata/train/YFT',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(NoF_path)\n",
    "NoF_images=[os.path.join('inputdata/train/NoF',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "images=os.listdir(OTHER_path)\n",
    "OTHER_images=[os.path.join('inputdata/train/OTHER',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "Allimages={'ALB': ALB_images,\n",
    "           'BET': BET_images,\n",
    "           'DOL': DOL_images,\n",
    "           'LAG': LAG_images,\n",
    "           'SHARK': SHARK_images,\n",
    "           'YFT': YFT_images,\n",
    "           'NoF': NoF_images,\n",
    "           'OTHER': OTHER_images}\n",
    "\n",
    "\n",
    "\n",
    "# Now getting all the test data\n",
    "testimages=[os.path.join('inputdata/test_stg1',ff) for ff in os.listdir('inputdata/test_stg1') if '.jpg' in ff ]+ \\\n",
    "            [os.path.join('inputdata/test_stg2',ff) for ff in os.listdir('inputdata/test_stg2') if '.jpg' in ff ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the Neural Network models to be run\n",
    "class NNmodels(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 500\n",
    "        self.nb_epoch = 1000\n",
    "        self.random_state = 51\n",
    "        self.listofmodels=['NNLinear-1h-500','NNSigmoid-1h-500','NNtanh-1h-500','NNRelu-1h-500','NNLeakyRelu-1h-500']\n",
    "        self.input_dim=None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        i=0\n",
    "        # Single Layer\n",
    "        for l2reg in np.linspace(0,1,10):\n",
    "            for N1layer in np.arange(100,1000,100):\n",
    "                for dropout in np.linspace(0,1,10):\n",
    "                    i=i+1\n",
    "                    if i>10:\n",
    "                        raise StopIteration\n",
    "                        \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"linear\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NN_linear_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'linear'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"sigmoid\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NN_sigmoid_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'sigmoid'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"tanh\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NN_tanh_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'tanh'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(Activation(\"relu\"))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NN_relu_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'relu'],model)\n",
    "                    \n",
    "                    #----------------------------------------\n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "                    model.add(LeakyReLU(alpha = 0.1))\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "                    model.add(Dense(output_dim=8))\n",
    "                    model.add(Activation(\"softmax\"))\n",
    "\n",
    "                    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "                    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    name='NN_LeakyReLU_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "                    yield (name,[N1layer,l2reg,dropout,'LeakyReLU'],model)\n",
    "                    \n",
    "                    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract feature and train on different models\n",
    "featureExtractors=[\n",
    "                     {'name':'ResNet50','model':ResNet50(weights='imagenet')},\n",
    "                     {'name':'VGG16','model':VGG16(weights='imagenet', include_top=True)},\n",
    "                     {'name':'VGG19','model':VGG19(weights='imagenet', include_top=True)},\n",
    "                     {'name':'InceptionV3','model':InceptionV3(input_tensor=Input(shape=(224, 224, 3)) ,weights='imagenet', include_top=True)}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepFeatureExtractClassify(object):\n",
    "    \n",
    "    def __init__(self,Allimages=[],testimages=[],\n",
    "                     featureExtractors=featureExtractors,  classifiers=['SVC','RandomForestClassifier','LinearSVC','XGBOOST']):\n",
    "        gc.collect()\n",
    "        self.featureExtractors=featureExtractors\n",
    "        self.classifiers=classifiers\n",
    "        self.Allimages=Allimages\n",
    "        self.testimages=testimages\n",
    "        data=np.load('imagenetlabels.npz')\n",
    "        self.ImageNetLabels=data['labels']\n",
    "        \n",
    "        self.NNmodels=NNmodels()\n",
    "        \n",
    "    def __del__(self):\n",
    "        del self.Xmean\n",
    "        del self.Xext\n",
    "        \n",
    "    def GetImagesMean(self):\n",
    "        if not os.path.isfile('TrainImageMean.npz'):\n",
    "\n",
    "            mean_R = []\n",
    "            mean_G = []\n",
    "            mean_B = []\n",
    "\n",
    "            #loading images\n",
    "            for imageset in self.Allimages.keys():\n",
    "                print imageset\n",
    "                for img_path in Allimages[imageset]:\n",
    "                    img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "\n",
    "                    #converting images to arrays\n",
    "                    x = image.img_to_array(img)\n",
    "\n",
    "                    #finding the mean image\n",
    "                    a = np.mean(x[:,:,0])\n",
    "                    mean_R = np.append(mean_R,a)\n",
    "                    b = np.mean(x[:,:,1])\n",
    "                    mean_G = np.append(mean_G,b)\n",
    "                    c = np.mean(x[:,:,2])\n",
    "                    mean_B = np.append(mean_B,c)\n",
    "\n",
    "            #Mean Image\n",
    "            self.I_R = np.mean(mean_R)\n",
    "            self.I_G = np.mean(mean_G)\n",
    "            self.I_B = np.mean(mean_B)\n",
    "            print (self.I_R,self.I_G,self.I_B)\n",
    "            np.savez(\"TrainImageMean\",I_R=self.I_R,I_G=self.I_G,I_B=self.I_B)\n",
    "        else:\n",
    "            print \"loading saved data\"\n",
    "            data=np.load('TrainImageMean.npz')\n",
    "            self.I_R=data['I_R']\n",
    "            self.I_G=data['I_G']\n",
    "            self.I_B=data['I_B']\n",
    "            print (self.I_R,self.I_G,self.I_B)\n",
    "    \n",
    "    def SetTestData(self):\n",
    "        if not os.path.isfile('ModifiedTestData.npz'):\n",
    "            self.testfilenames=None\n",
    "            self.Xtestsubmit=None\n",
    "            for i,img_path in enumerate(self.testimages):\n",
    "                print \"Mean removing for test image \",str(i),\" of \", str( len(self.testimages) ),\"\\r\",\n",
    "                img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "                x = image.img_to_array(img)\n",
    "                x[:,:,0] = x[:,:,0] - self.I_R\n",
    "                x[:,:,1] = x[:,:,1] - self.I_G\n",
    "                x[:,:,2] = x[:,:,2] - self.I_B\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "\n",
    "                if self.Xtestsubmit is None:\n",
    "                    self.Xtestsubmit=x\n",
    "                    self.testfilenames=np.array([img_path])\n",
    "                else:\n",
    "                    self.Xtestsubmit=np.vstack((self.Xtestsubmit,x) )\n",
    "                    self.testfilenames=np.hstack((self.testfilenames,np.array([img_path])) )\n",
    "            np.savez('ModifiedTestData',Xtestsubmit=self.Xtestsubmit,testfilenames=self.testfilenames)\n",
    "        else:\n",
    "            print \"loading saved test data\"\n",
    "            data=np.load('ModifiedTestData.npz')\n",
    "            self.Xtestsubmit=data['Xtestsubmit']\n",
    "            self.testfilenames=data['testfilenames']\n",
    "            \n",
    "    def SetMeanRemovedData(self):\n",
    "        if not os.path.isfile('MeanRemovedData.npz'):\n",
    "            print \"Removing mean from images\"\n",
    "            self.Xmean=None\n",
    "            self.Y=None\n",
    "            for imageset in self.Allimages.keys():\n",
    "                print imageset\n",
    "                for img_path in Allimages[imageset]:\n",
    "                    img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "\n",
    "                    x = image.img_to_array(img)\n",
    "                    x[:,:,0] = x[:,:,0] - self.I_R\n",
    "                    x[:,:,1] = x[:,:,1] - self.I_G\n",
    "                    x[:,:,2] = x[:,:,2] - self.I_B\n",
    "\n",
    "                    x = np.expand_dims(x, axis=0)\n",
    "                    if self.Xmean is None:\n",
    "                        self.Xmean=x\n",
    "                        self.Y=np.array([imageset])\n",
    "                    else:\n",
    "                        self.Xmean=np.vstack((self.Xmean,x) )\n",
    "                        self.Y=np.vstack((self.Y,np.array([imageset])) )\n",
    "\n",
    "            self.Labels=np.unique(self.Y)\n",
    "            Y=self.Y\n",
    "            for ind,l in enumerate(self.Labels):\n",
    "                Y[np.argwhere(Y==l)]=ind\n",
    "            self.Ybinary=np_utils.to_categorical(Y)\n",
    "            np.savez('MeanRemovedData',Xmean=self.Xmean,Y=self.Y,Labels=self.Labels,Ybinary=self.Ybinary)\n",
    "        else:\n",
    "            print \"Loading mean removed data\"\n",
    "            data=np.load('MeanRemovedData.npz')\n",
    "            self.Xmean=data['Xmean']\n",
    "            self.Y=data['Y']\n",
    "            self.Y=self.Y.astype(int)\n",
    "                    \n",
    "            self.Labels=data['Labels']\n",
    "            self.Ybinary=data['Ybinary']\n",
    "            \n",
    "    def ExtractFeatures(self,rerun=False):\n",
    "        if not os.path.isfile('ExtractedFeatures.npz') or rerun:\n",
    "#             self.SetMeanRemovedData()\n",
    "            self.Xext={}\n",
    "            for model in featureExtractors:\n",
    "                print \"extracting features using deep covnet \",model['name']\n",
    "                X=None\n",
    "                for i in range( self.Xmean.shape[0] ):\n",
    "                    print \"Working on image ... \"+str(i)+\" of \"+ str(self.Xmean.shape[0]) +\"\\r\",\n",
    "                    x = np.expand_dims(self.Xmean[i], axis=0)\n",
    "                    preds = model['model'].predict(x)\n",
    "                    if X is None:\n",
    "                        X=preds[0]\n",
    "                    else:\n",
    "                        X=np.vstack( (X,preds[0]) )\n",
    "\n",
    "                self.Xext[model['name'] ]=X\n",
    "                print \"\\nDone\\n\"\n",
    "                \n",
    "            np.savez('ExtractedFeatures',Xext=self.Xext)\n",
    "        else:\n",
    "            print \"Loading Extracted Features\"\n",
    "            data=np.load('ExtractedFeatures.npz')\n",
    "            self.Xext=data['Xext'][()]\n",
    "        \n",
    "    def GetSplitTrain(self,X,y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                            X, y, test_size=0.33, random_state=41)\n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def RunNNmodels(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "        self.NNmodels.input_dim=X_train.shape[1]\n",
    "        for name,paras,model in self.NNmodels:\n",
    "            print \"\\n\\n<<<<<<<< ++  >>>>>>>>>>>\"\n",
    "            print \"NN model :: \",name\n",
    "            model.fit(X_train, np_utils.to_categorical(y_train), batch_size=self.NNmodels.batch_size, \n",
    "                      nb_epoch=self.NNmodels.nb_epoch,verbose=0, validation_split=0.3)\n",
    "            score = model.evaluate(X_test,np_utils.to_categorical(y_test), verbose=1)\n",
    "            print \"\"\n",
    "            print '\\n\\n Test score:', score\n",
    "            \n",
    "            self.Results[featuremodel['name']][name]={ 'clfs':[model],'paras':[paras] }\n",
    "            \n",
    "            dumpname=os.path.join(self.modelspath,  featuremodel['name']+'_'+name+'.h5')\n",
    "            model.save(dumpname)\n",
    "            self.ResultsSave[featuremodel['name']]['XGBOOST']={ 'clfs':[dumpname],'paras':[paras]  }\n",
    "        \n",
    "            \n",
    "            \n",
    "    def RunXGBoost(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train.reshape(-1,1))\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "        param = {'max_depth':100, 'eta':0.02, 'silent':1, 'objective':'multi:softmax','num_class':8 }\n",
    "        param['nthread'] = 6\n",
    "        param['eval_metric'] = 'mlogloss'\n",
    "        param['subsample'] = 0.7\n",
    "        param['colsample_bytree']= 0.7\n",
    "        param['min_child_weight'] = 0\n",
    "        param['booster'] = \"gblinear\"\n",
    "\n",
    "        watchlist  = [(dtrain,'train')]\n",
    "        num_round = 300\n",
    "        early_stopping_rounds=10\n",
    "\n",
    "        clf_xgb = xgb.train(param, dtrain, num_round, watchlist,early_stopping_rounds=early_stopping_rounds,verbose_eval = False)\n",
    "\n",
    "        Ypred = clf_xgb.predict(dtest)\n",
    "        # y_test=np_utils.to_categorical(y_test)\n",
    "\n",
    "        print \"\\n++ Accuracy Score ++\\n\"\n",
    "        print metrics.accuracy_score(y_test,Ypred)\n",
    "        print \"\\n++ Classification report ++\\n\"\n",
    "        print metrics.classification_report(y_test,Ypred)\n",
    "        print \"\\n++ Confusion Matrix ++\\n\"\n",
    "        print '\\x1b[1;31m'+ str(metrics.confusion_matrix(y_test,Ypred) )+'\\x1b[0m'\n",
    "        \n",
    "        self.Results[featuremodel['name']]['XGBoost']={ 'clfs':[clf_xgb] }\n",
    "        \n",
    "        dumpname=os.path.join(self.modelspath, featuremodel['name']+'_'+'XGBOOST.model')\n",
    "        clf_xgb.save_model(dumpname)\n",
    "        self.ResultsSave[featuremodel['name']]['XGBOOST']={ 'clfs':[dumpname] }\n",
    "    \n",
    "                    \n",
    "    \n",
    "                    \n",
    "    def RunClassifiers(self):\n",
    "        \"\"\"\n",
    "        Run all the classifiers with cross validation and grid search\n",
    "        \n",
    "        \"\"\"\n",
    "        self.version=str( datetime.datetime.now()).split('.')[0]\n",
    "        if not os.path.isdir('models_'+self.version):\n",
    "            os.mkdir('models_'+self.version)\n",
    "        self.modelspath='models_'+self.version\n",
    "        self.Results={}\n",
    "        self.ResultsSave={}\n",
    "         \n",
    "        \n",
    "\n",
    "            \n",
    "        for featuremodel in featureExtractors:\n",
    "            print \"running classification on features extracted by \", featuremodel['name']\n",
    "\n",
    "            X_train, X_test, y_train, y_test=self.GetSplitTrain(self.Xext[featuremodel['name'] ],self.Y)\n",
    "\n",
    "            ModelParaGrid=[{'name':'LinearSVC','model':LinearSVC(C=1),'para':{'C': [1, 10, 100, 1000,10000]}},\n",
    "                           {'name':'SVC','model':SVC(C=1.0, kernel='linear',probability=True,shrinking=True),'para':{'C': [1, 10, 100, 1000,10000]} },\n",
    "                            {'name':'LogisticRegression','model':LogisticRegression(C=1e1,n_jobs=5,verbose=1),\n",
    "                             'para':{'C': [1, 10, 100, 1000,10000]}},\n",
    "                            {'name':'RandomForestClassifier','model':RandomForestClassifier(n_estimators=50,\n",
    "                                                                n_jobs=2,max_depth=1000),\n",
    "                             'para':{'n_estimators':[10,50,100,150,250,500],'max_depth':[10,100,250,500,750,1000,1500]}},\n",
    "                          ]\n",
    "            self.Results[featuremodel['name']]={}\n",
    "            self.ResultsSave[featuremodel['name']]={}\n",
    "            \n",
    "            print \"#####################################################################################\"\n",
    "            print \"--------------------  \"+ featuremodel['name'] +\"  -----------------------------------\"\n",
    "            print \"#####################################################################################\"\n",
    "\n",
    "\n",
    "            for M in ModelParaGrid:\n",
    "                scores = ['precision', 'recall']\n",
    "                print \"************ \" + M['name'] + \"******************\"\n",
    "\n",
    "                self.Results[featuremodel['name']][M['name']]={}\n",
    "                self.Results[featuremodel['name']][M['name']]['clfs']=[]\n",
    "                \n",
    "                self.ResultsSave[featuremodel['name']][M['name']]={}\n",
    "                self.ResultsSave[featuremodel['name']][M['name']]['clfs']=[]\n",
    "                \n",
    "#                 continue\n",
    "                for score in scores:\n",
    "                    print \"# Tuning hyper-parameters for %s\" % score\n",
    "                    print \"\"\n",
    "\n",
    "                    clf = GridSearchCV(M['model'], M['para'], cv=5,\n",
    "                                       scoring='%s_macro' % score,n_jobs=5)\n",
    "                    clf.fit(X_train,  y_train.reshape(1,-1)[0])\n",
    "\n",
    "                    print \"Best parameters set found on development set:\"\n",
    "                    print \n",
    "                    print clf.best_params_\n",
    "                    print \n",
    "                    print \"Grid scores on development set:\"\n",
    "                    print\n",
    "                    means = clf.cv_results_['mean_test_score']\n",
    "                    stds = clf.cv_results_['std_test_score']\n",
    "                    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "                        print \"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params) \n",
    "                    print\n",
    "\n",
    "                    print \"Detailed classification report:\"\n",
    "                    print\n",
    "                    print \"The model is trained on the full development set.\"\n",
    "                    print \"The scores are computed on the full evaluation set.\"\n",
    "                    print\n",
    "                    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "                    print classification_report(y_true, y_pred)\n",
    "                   \n",
    "                    print\n",
    "                    self.Results[featuremodel['name']][M['name']]['clfs'].append( clf )\n",
    "                    \n",
    "                    dumpname=os.path.join(self.modelspath, featuremodel['name']+'_'+M['name']+'_'+score+'.pkl')\n",
    "                    joblib.dump(clf, dumpname)\n",
    "                    self.ResultsSave[featuremodel['name']][M['name']]['clfs'].append(dumpname)\n",
    "                    \n",
    "            # XGBoost\n",
    "            print '-----------------  XGBOOST - tree  ------------------------------'\n",
    "            self.RunXGBoost(featuremodel,X_train, X_test, y_train, y_test)\n",
    "            \n",
    "            # Running keras models on extracted features\n",
    "            print \"----------------- Keras NN models ----------------------------------\"\n",
    "            self.RunNNmodels(featuremodel,X_train, X_test, y_train, y_test)\n",
    "\n",
    "        \n",
    "            Resultsfile=os.path.join(self.modelspath, 'Results.json')\n",
    "            with open(Resultsfile,'w') as F:\n",
    "                json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "            \n",
    "    def LoadModels(self,modeldir=None):\n",
    "        if modeldir is None:\n",
    "            modeldirectories=[ff for ff in os.listdir('.') if os.path.isdir(ff) and 'model' in ff]\n",
    "        \n",
    "        print modeldir\n",
    "        Resultsfile=os.path.join( modeldir , 'Results.json')\n",
    "        with open(Resultsfile,'r') as F:\n",
    "            self.ResultsSave=json.load(F)\n",
    "        \n",
    "        self.Results=self.ResultsSave\n",
    "        for FM in self.Results.keys():\n",
    "            for clftype in self.Results[FM].keys():\n",
    "                for i in range( len(self.Results[FM][clftype]['clfs']) ):\n",
    "                    ff=self.Results[FM][clftype]['clfs'][i]\n",
    "                    if '.pkl' in ff:\n",
    "                        clf = joblib.load(ff)\n",
    "                    elif '.model' in ff:\n",
    "                        clf = xgb.Booster({'nthread':4}) #init model\n",
    "                        clf.load_model(ff) # load data\n",
    "                    elif '.h5' in ff:\n",
    "                        clf = load_model(ff)\n",
    "                    else:\n",
    "                        print \"Model name not knwon\"\n",
    "                        clf=None\n",
    "                    self.Results[FM][clftype]['clfs'][i]=clf\n",
    "    \n",
    "    def EvalPerformance(self):\n",
    "        print \"Generating random test data\"\n",
    "        X_train, X_test, y_train, y_test=self.GetSplitTrain(self.Xmean,self.Y)\n",
    "        \n",
    "        print \"Evaluating performance\"\n",
    "        self.PerFormance=[]\n",
    "        for FM in self.Results.keys():\n",
    "            # get the model to extract features\n",
    "            for featuremodel in featureExtractors:\n",
    "                if featuremodel['name']==FM:\n",
    "                    break\n",
    "            if featuremodel['name']!=FM:\n",
    "                raise \"Error no mfeatjure model match\"\n",
    "            model=featuremodel['model']\n",
    "            X=model.predict(X_test)\n",
    "                    \n",
    "            for clftype in self.Results[FM].keys():\n",
    "                for i in range( len(self.Results[FM][clftype]['clfs']) ):\n",
    "                    clf=self.Results[FM][clftype]['clfs'][i]\n",
    "                    if hasattr(clf,'predict_proba'):\n",
    "                        y_pred=clf.predict_proba(X)\n",
    "                    else:\n",
    "                        y_pred=clf.predict(X)\n",
    "                        \n",
    "                    logloss=log_loss(y_test, y_pred, eps=1e-15, normalize=True)\n",
    "                    self.PerFormance.append( {'FeatureExtractor': FM, \n",
    "                                              'Classifier':clftype+'_'+str(i), \n",
    "                                              'log_loss' : logloss\n",
    "                                             } )\n",
    "        df=pd.DataFrame(self.PerFormance)\n",
    "        df.sort_values(by=log_loss,ascending=False,inplace=True)\n",
    "        \n",
    "        print df[['FeatureExtractor','Classifier','log_loss']]\n",
    "        \n",
    "# randomforrest : predict_proba\n",
    "# SVC : predict_proba\n",
    "# LinearSVC: predict\n",
    "# LogisticRegression : predict_proba\n",
    "# Xgboost  : predict_proba, predict\n",
    "# NN : predict_proba, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del DFEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DFEC=DeepFeatureExtractClassify(Allimages=Allimages,testimages=testimages)\n",
    "# DFEC.GetImagesMean()\n",
    "# DFEC.SetTestData()\n",
    "\n",
    "# DFEC.SetMeanRemovedData()\n",
    "DFEC.ExtractFeatures(rerun=False)\n",
    "DFEC.RunClassifiers(rerun=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_2017-04-20 00:53:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AttributeError: 'Xmean' in <bound method DeepFeatureExtractClassify.__del__ of <__main__.DeepFeatureExtractClassify object at 0x7f3f4b1c4950>> ignored\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON object could be decoded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1a0bef37903a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDFEC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeepFeatureExtractClassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAllimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAllimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDFEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models_2017-04-20 00:53:13'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-9b50cf8c749d>\u001b[0m in \u001b[0;36mLoadModels\u001b[0;34m(self, modeldir)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mResultsfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodeldir\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'Results.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResultsfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResultsSave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResultsSave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mparse_constant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         **kw)\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No JSON object could be decoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON object could be decoded"
     ]
    }
   ],
   "source": [
    "DFEC=DeepFeatureExtractClassify(Allimages=Allimages,testimages=testimages)\n",
    "DFEC.LoadModels(modeldir='models_2017-04-20 00:53:13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
