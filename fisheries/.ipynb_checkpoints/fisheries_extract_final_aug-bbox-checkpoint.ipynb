{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding box based classifications\n",
    "\n",
    "## Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "from __future__ import division\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "# # import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import types    \n",
    "import PIL\n",
    "from keras.models import load_model\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import __version__ as keras_version\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU,PReLU\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import average_precision_score,accuracy_score,recall_score,precision_score\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import pdb\n",
    "\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn import linear_model,decomposition\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression,SGDClassifier\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack ,vstack\n",
    "\n",
    "import gc\n",
    "\n",
    "print(gc.collect())\n",
    "\n",
    "import xgboost as xgb\n",
    "from keras import applications\n",
    "import os\n",
    "\n",
    "\n",
    "# deep models\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import multiprocessing as mltproc\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet50.name='ResNet50'\n",
    "# VGG16.name='VGG16'\n",
    "# VGG19.name='VGG19'\n",
    "# InceptionV3.name='InceptionV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Listallimages=[]\n",
    "# ALB_path='inputdata/train/ALB'\n",
    "# BET_path='inputdata/train/BET'\n",
    "# DOL_path='inputdata/train/DOL'\n",
    "# LAG_path='inputdata/train/LAG'\n",
    "# SHARK_path='inputdata/train/SHARK'\n",
    "# YFT_path='inputdata/train/YFT'\n",
    "# NoF_path = 'inputdata/train/NoF'\n",
    "# OTHER_path = 'inputdata/train/OTHER'\n",
    "\n",
    "\n",
    "# images=os.listdir(ALB_path)\n",
    "# ALB_images=[os.path.join('inputdata/train/ALB',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "\n",
    "# images=os.listdir(BET_path)\n",
    "# BET_images=[os.path.join('inputdata/train/BET',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(DOL_path)\n",
    "# DOL_images=[os.path.join('inputdata/train/DOL',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(LAG_path)\n",
    "# LAG_images=[os.path.join('inputdata/train/LAG',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(SHARK_path)\n",
    "# SHARK_images=[os.path.join('inputdata/train/SHARK',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(YFT_path)\n",
    "# YFT_images=[os.path.join('inputdata/train/YFT',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(NoF_path)\n",
    "# NoF_images=[os.path.join('inputdata/train/NoF',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(OTHER_path)\n",
    "# OTHER_images=[os.path.join('inputdata/train/OTHER',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# Allimages={'ALB': ALB_images,\n",
    "#            'BET': BET_images,\n",
    "#            'DOL': DOL_images,\n",
    "#            'LAG': LAG_images,\n",
    "#            'SHARK': SHARK_images,\n",
    "#            'YFT': YFT_images,\n",
    "#            'NoF': NoF_images,\n",
    "#            'OTHER': OTHER_images}\n",
    "\n",
    "\n",
    "\n",
    "# # Now getting all the test data\n",
    "# testimages=[os.path.join('inputdata/test_stg1',ff) for ff in os.listdir('inputdata/test_stg1') if '.jpg' in ff ]+ \\\n",
    "#             [os.path.join('inputdata/test_stg2',ff) for ff in os.listdir('inputdata/test_stg2') if '.jpg' in ff ]\n",
    "\n",
    "\n",
    "    \n",
    "# # getting cropped images\n",
    "# ALB_cropped_path='inputdata/train/ALB/cropped'\n",
    "# BET_cropped_path='inputdata/train/BET/cropped'\n",
    "# DOL_cropped_path='inputdata/train/DOL/cropped'\n",
    "# LAG_cropped_path='inputdata/train/LAG/cropped'\n",
    "# SHARK_cropped_path='inputdata/train/SHARK/cropped'\n",
    "# YFT_cropped_path='inputdata/train/YFT/cropped'\n",
    "# NoF_cropped_path = 'inputdata/train/NoF/cropped'\n",
    "# OTHER_cropped_path = 'inputdata/train/OTHER/cropped'\n",
    "\n",
    "# images=os.listdir(ALB_cropped_path)\n",
    "# ALB_cropped_images=[os.path.join(ALB_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(BET_cropped_path)\n",
    "# BET_cropped_images=[os.path.join(BET_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(DOL_cropped_path)\n",
    "# DOL_cropped_images=[os.path.join(DOL_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(LAG_cropped_path)\n",
    "# LAG_cropped_images=[os.path.join(LAG_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(SHARK_cropped_path)\n",
    "# SHARK_cropped_images=[os.path.join(SHARK_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(YFT_cropped_path)\n",
    "# YFT_cropped_images=[os.path.join(YFT_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(NoF_cropped_path)\n",
    "# NoF_cropped_images=[os.path.join(NoF_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# images=os.listdir(OTHER_cropped_path)\n",
    "# OTHER_cropped_images=[os.path.join(OTHER_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "# Allimages_cropped={'ALB': ALB_cropped_images,\n",
    "#            'BET': BET_cropped_images,\n",
    "#            'DOL': DOL_cropped_images,\n",
    "#            'LAG': LAG_cropped_images,\n",
    "#            'SHARK': SHARK_cropped_images,\n",
    "#            'YFT': YFT_cropped_images,\n",
    "#            'NoF': NoF_cropped_images,\n",
    "#            'OTHER': OTHER_cropped_images}\n",
    "\n",
    "# fishes=Allimages.keys()\n",
    "# All_bboxes={}\n",
    "# for fish in fishes:\n",
    "#     All_bboxes[fish]=[]\n",
    "#     with open('boundingbox_'+fish+'.json','r') as ff:\n",
    "#         bboxes=json.load(ff)\n",
    "#     for img in Allimages[fish]:\n",
    "#         imagename=img.split('/')[-1]\n",
    "#         if imagename in bboxes.keys():\n",
    "#             All_bboxes[fish].append(bboxes[imagename])\n",
    "#         else:\n",
    "#             All_bboxes[fish].append([])\n",
    "            \n",
    "# # now get the original sizes of images\n",
    "# Allimages_shapes={}\n",
    "# for imageset in Allimages.keys():\n",
    "#     print imageset\n",
    "#     Allimages_shapes[imageset]=[]\n",
    "#     for img_path in Allimages[imageset]:\n",
    "#         img = image.load_img(img_path)\n",
    "\n",
    "#         #converting images to arrays\n",
    "#         x = image.img_to_array(img)\n",
    "#         Allimages_shapes[imageset].append(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savez('AllImagesData.npz',All_bboxes=All_bboxes,Allimages=Allimages,Allimages_cropped=Allimages_cropped,Allimages_shapes=Allimages_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load('AllImagesData.npz')\n",
    "All_bboxes=data['All_bboxes'][()]\n",
    "Allimages=data['Allimages'][()]\n",
    "Allimages_cropped=data['Allimages_cropped'][()]\n",
    "Allimages_shapes=data['Allimages_shapes'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['All_bboxes'][()]\n",
    "print len(All_bboxes['BET'])\n",
    "print len(Allimages['BET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract feature and train on different models\n",
    "featureExtractors=[\n",
    "                     {'name':'ResNet50','model':ResNet50(weights='imagenet')},\n",
    "                     {'name':'VGG16','model':VGG16(weights='imagenet', include_top=True)},\n",
    "#                      {'name':'VGG19','model':VGG19(weights='imagenet', include_top=True)},\n",
    "#                      {'name':'InceptionV3','model':InceptionV3(input_tensor=Input(shape=(224, 224, 3)) ,weights='imagenet', include_top=True)}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # All the Neural Network models to be run\n",
    "# class NNmodels(object):\n",
    "#     def __init__(self):\n",
    "#         self.batch_size = 500\n",
    "#         self.nb_epoch = 100\n",
    "#         self.random_state = 51\n",
    "#         self.listofmodels=['NNLinear-1h-500','NNSigmoid-1h-500','NNtanh-1h-500','NNRelu-1h-500','NNLeakyRelu-1h-500']\n",
    "#         self.input_dim=None\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         i=0\n",
    "#         # Single Layer\n",
    "#         for l2reg in [0]:\n",
    "#             for N1layer in [50,500]:\n",
    "#                 for dropout in np.arange(0,1,0.2):\n",
    "#                     i=i+1\n",
    "# #                     if i>5:\n",
    "# #                         raise StopIteration\n",
    "                        \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"linear\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNlinear_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'linear'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"sigmoid\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNsigmoid_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'sigmoid'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"tanh\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNtanh_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'tanh'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"relu\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNrelu_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'relu'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(LeakyReLU(alpha = 0.1))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNLeakyReLU_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'LeakyReLU'],model)\n",
    "        \n",
    "        \n",
    "#         # Single Layer\n",
    "#         for l2reg in np.linspace(0,0.1,5):\n",
    "#             for N1layer in [50,500]:\n",
    "#                 for dropout in [0]:\n",
    "#                     i=i+1\n",
    "# #                     if i>5:\n",
    "# #                         raise StopIteration\n",
    "                        \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"linear\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNlinear_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'linear'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"sigmoid\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNsigmoid_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'sigmoid'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"tanh\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNtanh_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'tanh'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(Activation(\"relu\"))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNrelu_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'relu'],model)\n",
    "                    \n",
    "#                     #----------------------------------------\n",
    "#                     model = Sequential()\n",
    "#                     model.add(Dense(output_dim=N1layer, input_dim=self.input_dim,kernel_regularizer=regularizers.l2(l2reg),))\n",
    "#                     model.add(LeakyReLU(alpha = 0.1))\n",
    "#                     model.add(Dropout(dropout))\n",
    "\n",
    "#                     model.add(Dense(output_dim=8))\n",
    "#                     model.add(Activation(\"softmax\"))\n",
    "\n",
    "#                     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.3, nesterov=True)\n",
    "#                     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "#                     name='NNLeakyReLU_%d_%02.4f_%02.4f'%(N1layer,l2reg,dropout)\n",
    "                    \n",
    "#                     yield (name,[N1layer,l2reg,dropout,'LeakyReLU'],model)\n",
    "                    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def chunks(l, n):\n",
    "#     \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "#     for i in range(0, len(l), n):\n",
    "#         yield l[i:i + n]\n",
    "                \n",
    "# class DeepFeatureExtractClassify(object):\n",
    "    \n",
    "#     def __init__(self,Allimages=[],testimages=[],\n",
    "#                      featureExtractors=featureExtractors,\n",
    "#                      TrainImageMean='TrainImageMean.npz',\n",
    "#                      ModifiedTestData='ModifiedTestData.npz',\n",
    "#                      MeanRemovedData='MeanRemovedData.npz',\n",
    "#                      ExtractedFeatures='ExtractedFeatures.npz'\n",
    "#                         ):\n",
    "        \n",
    "#         self.TrainImageMean=TrainImageMean\n",
    "#         self.ModifiedTestData=ModifiedTestData\n",
    "#         self.MeanRemovedData=MeanRemovedData\n",
    "#         self.ExtractedFeatures=ExtractedFeatures\n",
    "        \n",
    "#         self.featureExtractors=featureExtractors\n",
    "#         self.Allimages=Allimages\n",
    "#         self.testimages=testimages\n",
    "#         data=np.load('imagenetlabels.npz')\n",
    "#         self.ImageNetLabels=data['labels']\n",
    "        \n",
    "#         self.NNmodels=NNmodels()\n",
    "#         gc.collect()\n",
    "        \n",
    "# #     def __del__(self):\n",
    "# #         del self.Xmean\n",
    "# #         del self.Xext\n",
    "        \n",
    "#     def GetImagesMean(self):\n",
    "#         if not os.path.isfile(self.TrainImageMean):\n",
    "\n",
    "#             mean_R = []\n",
    "#             mean_G = []\n",
    "#             mean_B = []\n",
    "\n",
    "#             #loading images\n",
    "#             for imageset in self.Allimages.keys():\n",
    "#                 print imageset\n",
    "#                 for img_path in self.Allimages[imageset]:\n",
    "#                     img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "\n",
    "#                     #converting images to arrays\n",
    "#                     x = image.img_to_array(img)\n",
    "\n",
    "#                     #finding the mean image\n",
    "#                     a = np.mean(x[:,:,0])\n",
    "#                     mean_R = np.append(mean_R,a)\n",
    "#                     b = np.mean(x[:,:,1])\n",
    "#                     mean_G = np.append(mean_G,b)\n",
    "#                     c = np.mean(x[:,:,2])\n",
    "#                     mean_B = np.append(mean_B,c)\n",
    "\n",
    "#             #Mean Image\n",
    "#             self.I_R = np.mean(mean_R)\n",
    "#             self.I_G = np.mean(mean_G)\n",
    "#             self.I_B = np.mean(mean_B)\n",
    "#             print (self.I_R,self.I_G,self.I_B)\n",
    "#             np.savez(self.TrainImageMean,I_R=self.I_R,I_G=self.I_G,I_B=self.I_B)\n",
    "#         else:\n",
    "#             print \"loading saved mean data\"\n",
    "#             data=np.load(self.TrainImageMean)\n",
    "#             self.I_R=data['I_R']\n",
    "#             self.I_G=data['I_G']\n",
    "#             self.I_B=data['I_B']\n",
    "#             print (self.I_R,self.I_G,self.I_B)\n",
    "    \n",
    "#     def SetTestData(self):\n",
    "#         if not os.path.isfile(self.ModifiedTestData):\n",
    "#             self.testfilenames=None\n",
    "#             self.Xtestsubmit=None\n",
    "#             for i,img_path in enumerate(self.testimages):\n",
    "#                 print \"Mean removing for test image \",str(i),\" of \", str( len(self.testimages) ),\"\\r\",\n",
    "#                 img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "#                 x = image.img_to_array(img)\n",
    "#                 x[:,:,0] = x[:,:,0] - self.I_R\n",
    "#                 x[:,:,1] = x[:,:,1] - self.I_G\n",
    "#                 x[:,:,2] = x[:,:,2] - self.I_B\n",
    "#                 x = np.expand_dims(x, axis=0)\n",
    "\n",
    "#                 if self.Xtestsubmit is None:\n",
    "#                     self.Xtestsubmit=x\n",
    "#                     self.testfilenames=np.array([img_path])\n",
    "#                 else:\n",
    "#                     self.Xtestsubmit=np.vstack((self.Xtestsubmit,x) )\n",
    "#                     self.testfilenames=np.hstack((self.testfilenames,np.array([img_path])) )\n",
    "#             np.savez(self.ModifiedTestData,Xtestsubmit=self.Xtestsubmit,testfilenames=self.testfilenames)\n",
    "#         else:\n",
    "#             print \"loading saved test data\"\n",
    "#             data=np.load(self.ModifiedTestData)\n",
    "#             self.Xtestsubmit=data['Xtestsubmit']\n",
    "#             self.testfilenames=data['testfilenames']\n",
    "            \n",
    "    \n",
    "#     def SetMeanRemovedData(self,equalizeclasses=False):\n",
    "#         if not os.path.isfile(self.MeanRemovedData):\n",
    "#             datagen = ImageDataGenerator(\n",
    "#                     rotation_range=40,\n",
    "#                     width_shift_range=0.2,\n",
    "#                     height_shift_range=0.2,\n",
    "#                     shear_range=0.2,\n",
    "#                     zoom_range=0.2,\n",
    "#                     horizontal_flip=True,\n",
    "#                     vertical_flip=True,\n",
    "#                     fill_mode='nearest')\n",
    "\n",
    "\n",
    "#             print \"Removing mean from images\"\n",
    "#             Xmean=None\n",
    "#             Y=None\n",
    "#             for imageset in self.Allimages.keys():\n",
    "#                 print imageset,' = ',len( self.Allimages[imageset] )\n",
    "#                 for img_path in self.Allimages[imageset]:\n",
    "#                     try:\n",
    "#                         img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "#                     except:\n",
    "#                         print \"error reading image \", img_path\n",
    "#                         continue\n",
    "\n",
    "#                     x = image.img_to_array(img)\n",
    "#                     x[:,:,0] = x[:,:,0] - self.I_R\n",
    "#                     x[:,:,1] = x[:,:,1] - self.I_G\n",
    "#                     x[:,:,2] = x[:,:,2] - self.I_B\n",
    "\n",
    "#                     x = np.expand_dims(x, axis=0)\n",
    "#                     if Xmean is None:\n",
    "#                         Xmean=x\n",
    "#                         Y=np.array([imageset])\n",
    "#                     else:\n",
    "#                         Xmean=np.vstack((Xmean,x) )\n",
    "#                         Y=np.vstack((Y,np.array([imageset])) )\n",
    "\n",
    "#             try:\n",
    "#                 if equalizeclasses==False:\n",
    "#                     self.Xmean=Xmean\n",
    "#                     self.Y=Y\n",
    "#                     i=0\n",
    "#                     for batch in datagen.flow(Xmean,Y, batch_size=12):\n",
    "#                         i=i+1\n",
    "#                         if i>100:\n",
    "#                             break\n",
    "#                         self.Xmean=np.vstack((self.Xmean,batch[0]) )\n",
    "#                         self.Y=np.vstack((self.Y, batch[1] ) )\n",
    "#                 else:\n",
    "#                     self.Xmean=Xmean\n",
    "#                     self.Y=Y\n",
    "\n",
    "# #                     pdb.set_trace()\n",
    "\n",
    "#                     y=Y.reshape(1,-1)[0]\n",
    "#                     classsizes=[]\n",
    "#                     labels=np.unique(y)\n",
    "#                     for cls in labels:\n",
    "#                         classsizes.append(len(y[y==cls]))\n",
    "\n",
    "#                     classsizes=np.array(classsizes)\n",
    "#                     print classsizes\n",
    "#                     mxsz=1.3*max(classsizes)\n",
    "#                     meansz=1.3*np.mean(classsizes)\n",
    "#                     for clsind in range(len(classsizes)):\n",
    "#                         print labels[clsind],\" original size \",len(Y[y==labels[clsind]])\n",
    "#                         iters= int( (mxsz-classsizes[clsind])/20 ) /2\n",
    "#                         for batch in datagen.flow(Xmean[y==labels[clsind],:,:,:],Y[y==labels[clsind]], batch_size=20):\n",
    "#                             self.Xmean=np.vstack((self.Xmean,batch[0]) )\n",
    "#                             self.Y=np.vstack((self.Y, batch[1] ) )\n",
    "\n",
    "#                             iters=iters-1\n",
    "#                             print labels[clsind],clsind,classsizes[clsind],mxsz,iters,'\\r',\n",
    "#                             if iters<0:\n",
    "#                                 break\n",
    "#             except:\n",
    "#                 pdb.set_trace()\n",
    "\n",
    "\n",
    "#             # Now try over sample to make the data \n",
    "#             print \"original size  = \",Xmean.shape\n",
    "#             print \"augmented size  = \",self.Xmean.shape\n",
    "            \n",
    "#             self.Labels=np.unique(self.Y)\n",
    "#             Y=self.Y\n",
    "#             for ind,l in enumerate(self.Labels):\n",
    "#                 Y[np.argwhere(Y==l)]=ind\n",
    "#             self.Ybinary=np_utils.to_categorical(Y)\n",
    "#             self.Y=self.Y.astype(int)\n",
    "#             self.Ybinary=self.Ybinary.astype(int)\n",
    "#             np.savez(self.MeanRemovedData,Xmean=self.Xmean,Y=self.Y,Labels=self.Labels,Ybinary=self.Ybinary)\n",
    "#         else:\n",
    "#             print \"Loading mean removed data\"\n",
    "#             data=np.load(self.MeanRemovedData)\n",
    "#             self.Xmean=data['Xmean']\n",
    "#             self.Y=data['Y']\n",
    "#             self.Y=self.Y.astype(int)\n",
    "\n",
    "#             self.Labels=data['Labels']\n",
    "#             self.Ybinary=data['Ybinary']  \n",
    "            \n",
    "            \n",
    "            \n",
    "#     def ExtractFeatures(self,rerun=False):\n",
    "#         if not os.path.isfile(self.ExtractedFeatures) or rerun:\n",
    "# #             self.SetMeanRemovedData()\n",
    "#             self.Xext={}\n",
    "#             for model in featureExtractors:\n",
    "#                 print \"extracting features using deep covnet \",model['name']\n",
    "#                 X=None\n",
    "#                 for i in range( self.Xmean.shape[0] ):\n",
    "#                     print \"Working on image ... \"+str(i)+\" of \"+ str(self.Xmean.shape[0]) +\"\\r\",\n",
    "#                     x = np.expand_dims(self.Xmean[i], axis=0)\n",
    "#                     preds = model['model'].predict(x)\n",
    "#                     if X is None:\n",
    "#                         X=preds[0]\n",
    "#                     else:\n",
    "#                         X=np.vstack( (X,preds[0]) )\n",
    "\n",
    "#                 self.Xext[model['name'] ]=X\n",
    "#                 print \"\\nDone\\n\"\n",
    "                \n",
    "#             np.savez(self.ExtractedFeatures,Xext=self.Xext)\n",
    "#         else:\n",
    "#             print \"Loading Extracted Features\"\n",
    "#             data=np.load(self.ExtractedFeatures)\n",
    "#             self.Xext=data['Xext'][()]\n",
    "    \n",
    "    \n",
    "#     def ExtractFeatures_parallel(self,rerun=False):\n",
    "#         if not os.path.isfile(self.ExtractedFeatures) or rerun:\n",
    "# #             self.SetMeanRemovedData()\n",
    "#             self.Xext={}\n",
    "#             for model in featureExtractors:\n",
    "#                 print \"extracting features in parallel using deep covnet \",model['name']\n",
    "#                 X=None\n",
    "#                 for i in range( self.Xmean.shape[0] ):\n",
    "#                     print \"Working on image ... \"+str(i)+\" of \"+ str(self.Xmean.shape[0]) +\"\\r\",\n",
    "#                     x = np.expand_dims(self.Xmean[i], axis=0)\n",
    "#                     preds = model['model'].predict(x)\n",
    "#                     if X is None:\n",
    "#                         X=preds[0]\n",
    "#                     else:\n",
    "#                         X=np.vstack( (X,preds[0]) )\n",
    "\n",
    "#                 self.Xext[model['name'] ]=X\n",
    "#                 print \"\\nDone\\n\"\n",
    "                \n",
    "#             np.savez(self.ExtractedFeatures,Xext=self.Xext)\n",
    "#         else:\n",
    "#             print \"Loading Extracted Features\"\n",
    "#             data=np.load(self.ExtractedFeatures)\n",
    "#             self.Xext=data['Xext'][()]\n",
    "            \n",
    "#     def ConvertImage2BBoxes(self,imgarr,boxparas={'size':[(60,30),(30,60)]}):\n",
    "#         \"\"\"\n",
    "#         (w,h) box parameters\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "#     def GetSplitTrain(self,X,y,uniformizedata=False,N=41):\n",
    "#         if not uniformizedata:\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=N)\n",
    "#             return (X_train, X_test, y_train, y_test)\n",
    "               \n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=N)\n",
    "        \n",
    "#         return (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "#     def RunNNmodels_parallel(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "#         self.NNmodels.input_dim=X_train.shape[1]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "#         batches=[]\n",
    "#         for name,paras,model in self.NNmodels:\n",
    "            \n",
    "#             if name in self.Results[featuremodel['name']].keys():\n",
    "#                 if 'clfs' in self.Results[featuremodel['name']][name].keys():\n",
    "#                     if len(self.Results[featuremodel['name']][name]['clfs'])==1:\n",
    "#                         continue\n",
    "            \n",
    "                    \n",
    "#             batches.append((name,paras,model) )\n",
    "        \n",
    "#         def parallelruns(i,j):\n",
    "#             print i,j\n",
    "#             for name,paras,model in batches[i:j+1]:\n",
    "#                 print \"\\n\\n<<<<<<<< ++  >>>>>>>>>>>\"\n",
    "#                 print \"NN model :: \",name\n",
    "#                 model.fit(X_train, np_utils.to_categorical(y_train), batch_size=self.NNmodels.batch_size, \n",
    "#                           nb_epoch=self.NNmodels.nb_epoch,verbose=0, validation_split=0.3)\n",
    "#                 score = model.evaluate(X_test,np_utils.to_categorical(y_test), verbose=1)\n",
    "#                 print \"\"\n",
    "#                 print '\\n\\n Test score:', score\n",
    "\n",
    "#                 self.Results[featuremodel['name']][name]={ 'clfs':[model],'paras':[paras] }\n",
    "\n",
    "#                 dumpname=os.path.join(self.modelspath,  featuremodel['name']+'_'+name+'.h5')\n",
    "#                 model.save(dumpname)\n",
    "#                 self.ResultsSave[featuremodel['name']]['XGBOOST']={ 'clfs':[dumpname],'paras':[paras]  }\n",
    "        \n",
    "\n",
    "#         P=[]\n",
    "#         for chk in chunks(range(len(batches)),100):\n",
    "#             P.append( mltproc.Process(target=parallelruns, args=(chk[0],chk[-1],)) )\n",
    "\n",
    "#         for p in P:\n",
    "#             p.start()\n",
    "#         for p in P:\n",
    "#             p.join()\n",
    "\n",
    "#     def RunNNmodels(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "#         self.NNmodels.input_dim=X_train.shape[1]\n",
    "        \n",
    "#         i=1\n",
    "#         for name,paras,model in self.NNmodels:\n",
    "#             if name in self.Results[featuremodel['name']].keys():\n",
    "#                 if 'clfs' in self.Results[featuremodel['name']][name].keys():\n",
    "#                     if len(self.Results[featuremodel['name']][name]['clfs'])==1:\n",
    "#                         continue\n",
    "                    \n",
    "#             print \"\\n\\n<<<<<<<< ++  >>>>>>>>>>>\"\n",
    "#             print \"NN model :: \",name\n",
    "#             model.fit(X_train, np_utils.to_categorical(y_train), batch_size=self.NNmodels.batch_size, \n",
    "#                       nb_epoch=self.NNmodels.nb_epoch,verbose=0, validation_split=0.3)\n",
    "#             score = model.evaluate(X_test,np_utils.to_categorical(y_test), verbose=1)\n",
    "#             print \"\"\n",
    "#             print '\\n\\n Test score:', score\n",
    "\n",
    "#             self.Results[featuremodel['name']][name]={ 'clfs':[model],'paras':[paras] }\n",
    "\n",
    "#             dumpname= featuremodel['name']+'_'+name+'.h5'\n",
    "#             model.save(os.path.join(self.modelspath,  dumpname))\n",
    "#             self.ResultsSave[featuremodel['name']][name]={ 'clfs':[dumpname],'paras':[paras]  }\n",
    "\n",
    "#             if i%10==0:\n",
    "#                 with open(self.Resultsfile,'w') as F:\n",
    "#                     json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "#             i=i+1\n",
    "            \n",
    "#     def RunXGBoost(self,featuremodel,X_train, X_test, y_train, y_test):\n",
    "        \n",
    "#         if 'XGBOOST' in self.Results[featuremodel['name']].keys():\n",
    "#             if 'clfs' in self.Results[featuremodel['name']]['XGBOOST'].keys():\n",
    "#                 if len(self.Results[featuremodel['name']]['XGBOOST']['clfs'])==1:\n",
    "#                     return\n",
    "                    \n",
    "#         dtrain = xgb.DMatrix(X_train, label=y_train.reshape(-1,1))\n",
    "#         dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "#         param = {'max_depth':100, 'eta':0.02, 'silent':1, 'objective':'multi:softmax','num_class':8 }\n",
    "#         param['nthread'] = 6\n",
    "#         param['eval_metric'] = 'mlogloss'\n",
    "#         param['subsample'] = 0.7\n",
    "#         param['colsample_bytree']= 0.7\n",
    "#         param['min_child_weight'] = 0\n",
    "#         param['booster'] = \"gblinear\"\n",
    "\n",
    "#         watchlist  = [(dtrain,'train')]\n",
    "#         num_round = 300\n",
    "#         early_stopping_rounds=10\n",
    "\n",
    "#         clf_xgb = xgb.train(param, dtrain, num_round, watchlist,early_stopping_rounds=early_stopping_rounds,verbose_eval = False)\n",
    "\n",
    "#         Ypred = clf_xgb.predict(dtest)\n",
    "#         # y_test=np_utils.to_categorical(y_test)\n",
    "\n",
    "#         print \"\\n++ Accuracy Score ++\\n\"\n",
    "#         print metrics.accuracy_score(y_test,Ypred)\n",
    "#         print \"\\n++ Classification report ++\\n\"\n",
    "#         print metrics.classification_report(y_test,Ypred)\n",
    "#         print \"\\n++ Confusion Matrix ++\\n\"\n",
    "#         print '\\x1b[1;31m'+ str(metrics.confusion_matrix(y_test,Ypred) )+'\\x1b[0m'\n",
    "        \n",
    "#         self.Results[featuremodel['name']]['XGBoost']={ 'clfs':[clf_xgb] }\n",
    "        \n",
    "#         dumpname= featuremodel['name']+'_'+'XGBOOST.model'\n",
    "#         joblib.dump(clf_xgb, os.path.join(self.modelspath, dumpname) )\n",
    "# #         clf_xgb.save_model( os.path.join(self.modelspath, dumpname) )\n",
    "#         self.ResultsSave[featuremodel['name']]['XGBOOST']={ 'clfs':[dumpname] }\n",
    "    \n",
    "#         with open(self.Resultsfile,'w') as F:\n",
    "#             json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))            \n",
    "    \n",
    "                    \n",
    "#     def RunClassifiers(self,modeldir=None,skipdone=True):\n",
    "#         \"\"\"\n",
    "#         Run all the classifiers with cross validation and grid search\n",
    "        \n",
    "#         \"\"\"\n",
    "\n",
    "#         if modeldir==None:\n",
    "#             self.version=str( datetime.datetime.now()).split('.')[0]\n",
    "#             if not os.path.isdir('models_'+self.version):\n",
    "#                 os.mkdir('models_'+self.version)\n",
    "#             self.modelspath='models_'+self.version\n",
    "#         else:\n",
    "#             self.modelspath=modeldir\n",
    "    \n",
    "#         if skipdone==True:\n",
    "#             self.LoadModels(modeldir=modeldir)\n",
    "#         else:\n",
    "#             self.Results={}\n",
    "#             self.ResultsSave={}\n",
    "         \n",
    "#         self.Resultsfile=os.path.join(self.modelspath, 'Results.json')\n",
    "        \n",
    "        \n",
    "            \n",
    "#         for featuremodel in featureExtractors:\n",
    "#             print \"running classification on features extracted by \", featuremodel['name']\n",
    "\n",
    "#             X_train, X_test, y_train, y_test=self.GetSplitTrain(self.Xext[featuremodel['name'] ],self.Y)\n",
    "\n",
    "#             ModelParaGrid=[\n",
    "# #                             {'name':'LinearSVC','model':LinearSVC(C=1),'para':{'C': [1, 10, 100, 1000,10000]}},\n",
    "#                            {'name':'SVC','model':SVC(C=1.0, kernel='linear',max_iter=1e5,probability=True,shrinking=True),'para':{'C': [1, 10, 100, 1000,10000]} },\n",
    "# #                             {'name':'LogisticRegression','model':LogisticRegression(C=1e1,n_jobs=5,verbose=1),'para':{'C': [1, 10, 100, 1000,10000]}},\n",
    "#                             {'name':'RandomForestClassifier','model':RandomForestClassifier(n_estimators=15,\n",
    "#                                                                 n_jobs=5,max_depth=40,max_features=60),\n",
    "#                              'para':{'n_estimators':[10,100,250,500],'max_depth':[10,100,250,500], 'max_features': [30,40,50,60] }},\n",
    "#                             ]\n",
    "#             if featuremodel['name'] not in self.Results.keys():\n",
    "#                 self.Results[featuremodel['name']]={}\n",
    "#                 self.ResultsSave[featuremodel['name']]={}\n",
    "            \n",
    "#             print \"#####################################################################################\"\n",
    "#             print \"--------------------  \"+ featuremodel['name'] +\"  -----------------------------------\"\n",
    "#             print \"#####################################################################################\"\n",
    "\n",
    "\n",
    "#             for M in ModelParaGrid:\n",
    "#                 scores = ['precision', 'recall']\n",
    "#                 print \"************ \" + M['name'] + \"******************\"\n",
    "                \n",
    "#                 if M['name'] not in self.Results[featuremodel['name']].keys():\n",
    "#                     self.Results[featuremodel['name']][M['name']]={'clfs':[] }\n",
    "#                     self.ResultsSave[featuremodel['name']][M['name']]={'clfs':[]}\n",
    "\n",
    "#                 if len( self.Results[featuremodel['name']][M['name']]['clfs'] )==0:\n",
    "#                     for score in scores:\n",
    "#                         print \"# Tuning hyper-parameters for %s\" % score\n",
    "#                         print \"\"\n",
    "\n",
    "#                         clf = GridSearchCV(M['model'], M['para'], cv=3,\n",
    "#                                            scoring='%s_macro' % score,n_jobs=5)\n",
    "#                         clf.fit(X_train,  y_train.reshape(1,-1)[0])\n",
    "\n",
    "#                         print \"Best parameters set found on development set:\"\n",
    "#                         print \n",
    "#                         print clf.best_params_\n",
    "#                         print \n",
    "#                         print \"Grid scores on development set:\"\n",
    "#                         print\n",
    "#                         means = clf.cv_results_['mean_test_score']\n",
    "#                         stds = clf.cv_results_['std_test_score']\n",
    "#                         for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "#                             print \"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params) \n",
    "#                         print\n",
    "\n",
    "#                         print \"Detailed classification report:\"\n",
    "#                         print\n",
    "#                         print \"The model is trained on the full development set.\"\n",
    "#                         print \"The scores are computed on the full evaluation set.\"\n",
    "#                         print\n",
    "#                         y_true, y_pred = y_test, clf.predict(X_test)\n",
    "#                         print classification_report(y_true, y_pred)\n",
    "\n",
    "#                         print\n",
    "#                         self.Results[featuremodel['name']][M['name']]['clfs'].append( clf )\n",
    "\n",
    "#                         dumpname= featuremodel['name']+'_'+M['name']+'_'+score+'.pkl'\n",
    "#                         joblib.dump(clf, os.path.join(self.modelspath, dumpname) )\n",
    "#                         self.ResultsSave[featuremodel['name']][M['name']]['clfs'].append(dumpname)\n",
    "                        \n",
    "#                         with open(self.Resultsfile,'w') as F:\n",
    "#                             json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "                \n",
    "#             # XGBoost\n",
    "# #             print '-----------------  XGBOOST - tree  ------------------------------'\n",
    "# #             self.RunXGBoost(featuremodel,X_train, X_test, y_train, y_test)\n",
    "\n",
    "#             # Running keras models on extracted features\n",
    "# #             print \"----------------- Keras NN models ----------------------------------\"\n",
    "# #             self.RunNNmodels(featuremodel,X_train, X_test, y_train, y_test)\n",
    "# #             self.RunNNmodels_parallel(featuremodel,X_train, X_test, y_train, y_test)\n",
    "            \n",
    "        \n",
    "#             with open(self.Resultsfile,'w') as F:\n",
    "#                 json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "    \n",
    "#     def LoadModelfromfile(self,modeldir,ff):\n",
    "#         if '.pkl' in ff:\n",
    "#             clf = joblib.load(os.path.join(modeldir,ff) )\n",
    "#         elif '.model' in ff:\n",
    "#             clf = joblib.load(os.path.join(modeldir,ff))\n",
    "#             def predict_custom(self,X):\n",
    "#                 dtest = xgb.DMatrix(X)\n",
    "#                 y=self.predict(dtest)\n",
    "#                 return np_utils.to_categorical(y.reshape(-1,1))\n",
    "#             clf.predict_custom=types.MethodType( predict_custom, clf )\n",
    "\n",
    "#         elif '.h5' in ff:\n",
    "#             clf = load_model(os.path.join(modeldir,ff))\n",
    "#         else:\n",
    "#             print \"Model name not knwon\"\n",
    "#             clf=None\n",
    "#         return clf\n",
    "    \n",
    "    \n",
    "#     def LoadModels(self,modeldir):\n",
    "#         if modeldir is None:\n",
    "# #             modeldirectories=[ff for ff in os.listdir('.') if os.path.isdir(ff) and 'model' in ff]\n",
    "#             raise \"error error modeldir\"\n",
    "    \n",
    "#         print modeldir\n",
    "#         self.Resultsfile=os.path.join( modeldir , 'Results.json')\n",
    "#         if os.path.isfile(self.Resultsfile):\n",
    "#             with open(self.Resultsfile,'r') as F:\n",
    "#                 self.ResultsSave=json.load(F)\n",
    "            \n",
    "#             print \"loading saved models from the saved json\"\n",
    "            \n",
    "#             self.Results=copy.deepcopy(self.ResultsSave)\n",
    "#             print \"Lazy load ... model is loaded when predcition needed\"\n",
    "#             return \n",
    "        \n",
    "#             if not hasattr(self,'Results'):\n",
    "#                 self.Results={}\n",
    "    \n",
    "#             for FM in self.ResultsSave.keys():\n",
    "#                 if FM not in self.Results.keys():\n",
    "#                     self.Results[FM]={}\n",
    "\n",
    "#                 for clftype in self.ResultsSave[FM].keys():\n",
    "#                     if clftype not in self.Results[FM].keys():\n",
    "#                         self.Results[FM][clftype]={'clfs':[]}\n",
    "#                     if len( self.Results[FM][clftype]['clfs'] )==0:\n",
    "#                         print \"Loading model for \",FM,\" \",clftype,'\\r',\n",
    "                        \n",
    "#                         for i in range( len(self.ResultsSave[FM][clftype]['clfs']) ):\n",
    "#                             ff=self.ResultsSave[FM][clftype]['clfs'][i]\n",
    "#                             if '.pkl' in ff:\n",
    "#                                 clf = joblib.load(os.path.join(modeldir,ff) )\n",
    "#                             elif '.model' in ff:\n",
    "#     #                             clf = xgb.Booster({'nthread':4}) #init model\n",
    "#     #                             clf.load_model(os.path.join(modeldir,ff)) # load data\n",
    "#                                 clf = joblib.load(os.path.join(modeldir,ff))\n",
    "#                                 def predict_custom(self,X):\n",
    "#                                     dtest = xgb.DMatrix(X)\n",
    "#                                     y=self.predict(dtest)\n",
    "#                                     return np_utils.to_categorical(y.reshape(-1,1))\n",
    "#                                 clf.predict_custom=types.MethodType( predict_custom, clf )\n",
    "\n",
    "#                             elif '.h5' in ff:\n",
    "#                                 clf = load_model(os.path.join(modeldir,ff))\n",
    "#                             else:\n",
    "#                                 print \"Model name not knwon\"\n",
    "#                                 clf=None\n",
    "#                             self.Results[FM][clftype]['clfs'].append(clf)\n",
    "#         else:\n",
    "#             self.Results={}\n",
    "#             self.ResultsSave={}\n",
    "               \n",
    "#             # construct from the available\n",
    "#             print \"Results file not there\"\n",
    "#             i=0\n",
    "#             for model in os.listdir(modeldir):\n",
    "#                 FE=model.split('_')[0]\n",
    "#                 clfstr=\".\".join( \"_\".join(model.split('_')[1:]).split('.')[:-1] )\n",
    "#                 clfstr=clfstr.replace('_recall','').replace('_precision','')\n",
    "#                 i=i+1\n",
    "#                 print \"Loading model for \",model,\" \",i,\" of \",len(os.listdir(modeldir)),'\\r',\n",
    "                \n",
    "#                 if FE not in self.Results.keys():\n",
    "#                     self.Results[FE]={}\n",
    "#                     self.ResultsSave[FE]={}\n",
    "#                 if clfstr not in self.Results[FE].keys():\n",
    "#                     self.Results[FE][clfstr]={'clfs':[]}\n",
    "#                     self.ResultsSave[FE][clfstr]={'clfs':[]}\n",
    "\n",
    "#                 if '.pkl' in model:\n",
    "#                     clf = joblib.load(os.path.join(modeldir,model))\n",
    "#                 elif '.model' in model:\n",
    "# #                     clf = xgb.Booster({'nthread':4}) #init model\n",
    "# #                     clf.load_model(os.path.join(modeldir,model)) # load data\n",
    "#                     clf = joblib.load(os.path.join(modeldir,model))\n",
    "#                     def predict_custom(self,X):\n",
    "#                         dtest = xgb.DMatrix(X)\n",
    "#                         y=self.predict(dtest)\n",
    "#                         return np_utils.to_categorical(y.reshape(-1,1))\n",
    "#                     clf.predict_custom=types.MethodType( predict_custom, clf )\n",
    "                    \n",
    "#                 elif '.h5' in model:\n",
    "# #                     print os.path.join(modeldir,model)\n",
    "#                     clf = load_model(os.path.join(modeldir,model))\n",
    "#                 else:\n",
    "#                     print \"Model name not knwon\"\n",
    "#                     clf=None\n",
    "#                 self.Results[FE][clfstr]['clfs'].append(clf)\n",
    "#                 self.ResultsSave[FE][clfstr]['clfs'].append(model)\n",
    "                \n",
    "#             with open(self.Resultsfile,'w') as F:\n",
    "#                 json.dump(self.ResultsSave,F, indent=4, separators=(',', ': '))\n",
    "    \n",
    "#     def doprediction(self,clf,Xtest):\n",
    "        \n",
    "#         if hasattr(clf,'predict_custom'):\n",
    "#             y_pred=clf.predict_custom(Xtest)\n",
    "#         elif hasattr(clf,'predict_proba'):\n",
    "#             y_pred=clf.predict_proba(Xtest)\n",
    "#         else:\n",
    "#             y_pred=clf.predict(Xtest)\n",
    "#             try:\n",
    "#                 y_pred.shape[1]\n",
    "#             except:\n",
    "#                 y_pred=np_utils.to_categorical(y_pred.reshape(-1,1))\n",
    "\n",
    "#         return y_pred\n",
    "        \n",
    "#     def EvalPerformance(self,modeldir='',savetag=''):\n",
    "#         self.PerFormance=[]\n",
    "#         for N in range(15):\n",
    "\n",
    "#             print \"Generating random test data \",N\n",
    "#             train_idx, test_idx, _, _ = train_test_split(np.arange(self.Xmean.shape[0]), np.arange(self.Xmean.shape[0]), test_size=0.33, random_state=41+N)\n",
    "\n",
    "#             print \"Evaluating performance\"\n",
    "            \n",
    "#             for FM in self.Results.keys():\n",
    "#                 print \"--------- \",FM, \" -----------\"\n",
    "\n",
    "#                 Xtest=self.Xext[ FM ][test_idx,:]\n",
    "#                 Ytest=self.Y[test_idx]\n",
    "\n",
    "#                 for clftype in self.Results[FM].keys():\n",
    "#                     if 'NN' in clftype:\n",
    "#                         continue\n",
    "#                     for i in range( len(self.Results[FM][clftype]['clfs']) ):\n",
    "#                         print \"\\n\\n\"+FM+' '+clftype+' '+str(i)\n",
    "\n",
    "#                         clf=self.Results[FM][clftype]['clfs'][i]\n",
    "#                         if isinstance(clf, str) or isinstance(clf, unicode):\n",
    "#                             clf=self.LoadModelfromfile(modeldir,clf)\n",
    "#                             self.Results[FM][clftype]['clfs'][i]=clf\n",
    "                            \n",
    "#                         if hasattr(clf,'predict_custom'):\n",
    "#                             y_pred=clf.predict_custom(Xtest)\n",
    "#                         elif hasattr(clf,'predict_proba'):\n",
    "#                             y_pred=clf.predict_proba(Xtest)\n",
    "#                         else:\n",
    "#                             y_pred=clf.predict(Xtest)\n",
    "#                             try:\n",
    "#                                 y_pred.shape[1]\n",
    "#                             except:\n",
    "#                                 y_pred=np_utils.to_categorical(y_pred.reshape(-1,1))\n",
    "\n",
    "#                         logloss=log_loss(Ytest, y_pred, eps=1e-15, normalize=True)\n",
    "#                         avgprec= average_precision_score(np_utils.to_categorical(Ytest), y_pred)\n",
    "#                         acc= accuracy_score(Ytest, np.argmax(y_pred,axis=1) )\n",
    "#                         recallscore= recall_score(Ytest, np.argmax(y_pred,axis=1),average='micro' )\n",
    "#                         precisionscore= precision_score(Ytest, np.argmax(y_pred,axis=1),average='micro' )\n",
    "                        \n",
    "#                         classifierfamily=clftype\n",
    "#                         if '_NN' in classifierfamily:\n",
    "#                             classifierfamily='NN'\n",
    "#                         elif 'XGBOOST' in classifierfamily:\n",
    "#                             classifierfamily='XGBOOST'\n",
    "#                         elif 'LinearSVC' in classifierfamily:\n",
    "#                             classifierfamily='LinearSVC'\n",
    "#                         elif 'SVC' in classifierfamily:\n",
    "#                             classifierfamily='SVC'\n",
    "#                         elif 'RandomForestClassifier' in classifierfamily:\n",
    "#                             classifierfamily='RandomForestClassifier'\n",
    "#                         elif 'LogisticRegression' in classifierfamily:\n",
    "#                             classifierfamily='LogisticRegression'\n",
    "#                         else:\n",
    "#                             classifierfamily=None\n",
    "                        \n",
    "                            \n",
    "#                         self.PerFormance.append( {'FeatureExtractor': FM, \n",
    "#                                                   'Classifier':clftype+'_'+str(i), \n",
    "#                                                   'classifierfamily':classifierfamily,\n",
    "#                                                   'log_loss' : logloss,\n",
    "#                                                   'acc'   : acc,\n",
    "#                                                   'avgprec':avgprec,\n",
    "#                                                   'recallscore':recallscore,\n",
    "#                                                   'precisionscore':precisionscore,\n",
    "#                                                  } )\n",
    "#                         print \"\\r\",\n",
    "\n",
    "#         df=pd.DataFrame(self.PerFormance)\n",
    "#         df.sort_values(by='log_loss',ascending=True,inplace=True)\n",
    "\n",
    "#         print df[['FeatureExtractor','Classifier','log_loss','acc']]\n",
    "\n",
    "#         df.to_csv('Performance_'+savetag+'.csv')\n",
    "#         df.to_hdf('Performance_'+savetag+'.h5','table')\n",
    "\n",
    "\n",
    "#     def GenerateSubmission(self,top=10,modeldir=None,savetag=''):\n",
    "#         self.LoadModels(modeldir=modeldir)\n",
    "\n",
    "#         df=pd.read_hdf('Performance_'+savetag+'.h5','table')\n",
    "\n",
    "#         df.sort_values(by='log_loss',ascending=True,inplace=True)\n",
    "#         df=df[['FeatureExtractor','Classifier','log_loss','acc']].groupby(['FeatureExtractor','Classifier']).agg('mean').sort_values(by='log_loss',ascending=True)\n",
    "#         df=df.reset_index()\n",
    "\n",
    "#         for ind in df.index[0:top]:\n",
    "#             print df.loc[ind,:]\n",
    "#             FM=df.loc[ind,'FeatureExtractor']\n",
    "#             Classifier=df.loc[ind,'Classifier']\n",
    "#             clftype=\"_\".join( Classifier.split('_')[:-1] ) \n",
    "#             Classifier_index=int( Classifier.split('_')[-1] )\n",
    "\n",
    "#             clf=self.ResultsSave[ FM ][clftype]['clfs'][Classifier_index]\n",
    "#             if isinstance(clf, str) or isinstance(clf, unicode):\n",
    "#                 clf=self.LoadModelfromfile(modeldir,clf)\n",
    "#                 self.ResultsSave[ FM ][clftype]['clfs'][Classifier_index]=clf\n",
    "\n",
    "#             # first extract features using CNN\n",
    "#             testextdata = 'Extractedtestdata_'+FM+'.npz'\n",
    "#             Xext=None\n",
    "#             if not os.path.isfile(testextdata):\n",
    "#                 data=np.load(self.ModifiedTestData)\n",
    "#                 Xtestsubmit=data['Xtestsubmit']\n",
    "#                 testfilenames=data['testfilenames']\n",
    "\n",
    "#                 for model in featureExtractors:\n",
    "#                     if model['name']!=FM:\n",
    "#                         continue\n",
    "#                     print \"extracting features using deep covnet \",model['name']\n",
    "#                     X=None\n",
    "#                     for i in range( Xtestsubmit.shape[0] ):\n",
    "#                         print \"Working on image ... \"+str(i)+\" of \"+ str(Xtestsubmit.shape[0]) +\"\\r\",\n",
    "#                         x = np.expand_dims(Xtestsubmit[i], axis=0)\n",
    "#                         preds = model['model'].predict(x)\n",
    "#                         if Xext is None:\n",
    "#                             Xext=preds[0]\n",
    "#                         else:\n",
    "#                             Xext=np.vstack( (Xext,preds[0]) )\n",
    "\n",
    "#                     print \"\\nDone and saving\\n\"\n",
    "\n",
    "#                 np.savez(testextdata,Xext=Xext)\n",
    "#                 del Xtestsubmit\n",
    "\n",
    "#             else:\n",
    "#                 print \"loading saved data\"\n",
    "#                 data=np.load(testextdata)\n",
    "#                 Xext=data['Xext']\n",
    "\n",
    "#                 data=np.load(self.ModifiedTestData)\n",
    "#                 testfilenames=data['testfilenames']\n",
    "\n",
    "#             print \"doing prediction\"\n",
    "#             y_pred=self.doprediction(clf,Xext)\n",
    "#             dd=pd.DataFrame({'image':testfilenames, 'ALB': y_pred[:,0], 'BET': y_pred[:,1], 'DOL': y_pred[:,2], \n",
    "#                              'LAG': y_pred[:,3], 'NoF': y_pred[:,4], 'OTHER': y_pred[:,5], 'SHARK': y_pred[:,6], 'YFT': y_pred[:,7]})\n",
    "\n",
    "#             dd['image']=dd['image'].str.replace('inputdata/','')\n",
    "#             dd['image']=dd['image'].str.replace('test_stg1/','')\n",
    "#             dd.sort_values(by='image',inplace=True)\n",
    "#             dd[['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT']].to_csv('submission_'+FM+'_'+Classifier+'.csv',header=True,index=False)\n",
    "\n",
    "\n",
    "#             del Xext\n",
    "#             gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Run cropped - augmented data\n",
    "# DFEC=DeepFeatureExtractClassify(Allimages=Allimages_cropped,testimages=testimages,\n",
    "#                                         TrainImageMean='TrainImageMean_cropped.npz',\n",
    "#                                         ModifiedTestData='MeanRemovedTestData.npz',\n",
    "#                                         MeanRemovedData='MeanRemovedData_cropped.npz',\n",
    "#                                         ExtractedFeatures='ExtractedFeatures_cropped.npz')\n",
    "\n",
    "# DFEC.GetImagesMean()\n",
    "# DFEC.SetMeanRemovedData(equalizeclasses=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DFEC.ExtractFeatures(rerun=False)\n",
    "# DFEC.RunClassifiers(modeldir='models_cropped',skipdone=True)\n",
    "# # DFEC.EvalPerformance(modeldir='models_cropped',savetag='cropped')\n",
    "# # DFEC.GenerateSubmission(top=10,modeldir='models_cropped',savetag='cropped')\n",
    "# # df=pd.read_hdf('Performance_cropped.h5','table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('Meta.h5'):\n",
    "    ALB_path='inputdata/train/ALB'\n",
    "    BET_path='inputdata/train/BET'\n",
    "    DOL_path='inputdata/train/DOL'\n",
    "    LAG_path='inputdata/train/LAG'\n",
    "    SHARK_path='inputdata/train/SHARK'\n",
    "    YFT_path='inputdata/train/YFT'\n",
    "    NoF_path = 'inputdata/train/NoF'\n",
    "    OTHER_path = 'inputdata/train/OTHER'\n",
    "\n",
    "    images=os.listdir(ALB_path)\n",
    "    ALB_images=[os.path.join('inputdata/train/ALB',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(BET_path)\n",
    "    BET_images=[os.path.join('inputdata/train/BET',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(DOL_path)\n",
    "    DOL_images=[os.path.join('inputdata/train/DOL',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(LAG_path)\n",
    "    LAG_images=[os.path.join('inputdata/train/LAG',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(SHARK_path)\n",
    "    SHARK_images=[os.path.join('inputdata/train/SHARK',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(YFT_path)\n",
    "    YFT_images=[os.path.join('inputdata/train/YFT',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(NoF_path)\n",
    "    NoF_images=[os.path.join('inputdata/train/NoF',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(OTHER_path)\n",
    "    OTHER_images=[os.path.join('inputdata/train/OTHER',ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    Allimages={'ALB': ALB_images,\n",
    "               'BET': BET_images,\n",
    "               'DOL': DOL_images,\n",
    "               'LAG': LAG_images,\n",
    "               'SHARK': SHARK_images,\n",
    "               'YFT': YFT_images,\n",
    "               'NoF': NoF_images,\n",
    "               'OTHER': OTHER_images}\n",
    "\n",
    "\n",
    "\n",
    "    # getting cropped images\n",
    "    ALB_cropped_path='inputdata/train/ALB/cropped'\n",
    "    BET_cropped_path='inputdata/train/BET/cropped'\n",
    "    DOL_cropped_path='inputdata/train/DOL/cropped'\n",
    "    LAG_cropped_path='inputdata/train/LAG/cropped'\n",
    "    SHARK_cropped_path='inputdata/train/SHARK/cropped'\n",
    "    YFT_cropped_path='inputdata/train/YFT/cropped'\n",
    "    NoF_cropped_path = 'inputdata/train/NoF/cropped'\n",
    "    OTHER_cropped_path = 'inputdata/train/OTHER/cropped'\n",
    "\n",
    "    images=os.listdir(ALB_cropped_path)\n",
    "    ALB_cropped_images=[os.path.join(ALB_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(BET_cropped_path)\n",
    "    BET_cropped_images=[os.path.join(BET_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(DOL_cropped_path)\n",
    "    DOL_cropped_images=[os.path.join(DOL_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(LAG_cropped_path)\n",
    "    LAG_cropped_images=[os.path.join(LAG_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(SHARK_cropped_path)\n",
    "    SHARK_cropped_images=[os.path.join(SHARK_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(YFT_cropped_path)\n",
    "    YFT_cropped_images=[os.path.join(YFT_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(NoF_cropped_path)\n",
    "    NoF_cropped_images=[os.path.join(NoF_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    images=os.listdir(OTHER_cropped_path)\n",
    "    OTHER_cropped_images=[os.path.join(OTHER_cropped_path,ff) for ff in images if '.jpg' in ff ]\n",
    "\n",
    "    Allimages_cropped={'ALB': ALB_cropped_images,\n",
    "               'BET': BET_cropped_images,\n",
    "               'DOL': DOL_cropped_images,\n",
    "               'LAG': LAG_cropped_images,\n",
    "               'SHARK': SHARK_cropped_images,\n",
    "               'YFT': YFT_cropped_images,\n",
    "               'NoF': NoF_cropped_images,\n",
    "               'OTHER': OTHER_cropped_images}\n",
    "\n",
    "\n",
    "    dftrain_cropped=pd.DataFrame()\n",
    "    for imageset in Allimages_cropped.keys():\n",
    "        print imageset,' = ',len( Allimages_cropped[imageset] )\n",
    "        for img_path in Allimages_cropped[imageset]:\n",
    "            try:\n",
    "                img = image.load_img(img_path)\n",
    "            except:\n",
    "                print \"error reading image \", img_path\n",
    "                continue\n",
    "\n",
    "            imagename=img_path.split('/')[-1]\n",
    "\n",
    "            x = image.img_to_array(img)\n",
    "            df=pd.DataFrame({'imagename':imagename,\n",
    "                             'label':imageset,\n",
    "                            'imagepath':img_path,\n",
    "                            'xshape':x.shape[0],\n",
    "                            'yshape':x.shape[1]},index=[0])\n",
    "            dftrain_cropped=pd.concat([dftrain_cropped,df])\n",
    "\n",
    "\n",
    "    dftrain=pd.DataFrame()\n",
    "    for imageset in Allimages.keys():\n",
    "        print imageset,' = ',len( Allimages[imageset] )\n",
    "        with open('boundingbox_'+imageset+'.json','r') as ff:\n",
    "            bboxes=json.load(ff)\n",
    "\n",
    "        for img_path in Allimages[imageset]:\n",
    "            try:\n",
    "                img = image.load_img(img_path)\n",
    "            except:\n",
    "                print \"error reading image \", img_path\n",
    "                continue\n",
    "\n",
    "            imagename=img_path.split('/')[-1]\n",
    "\n",
    "            if imagename in bboxes.keys():\n",
    "                BB=bboxes[imagename]\n",
    "            else:\n",
    "                BB=[]\n",
    "\n",
    "\n",
    "\n",
    "            x = image.img_to_array(img)\n",
    "            df=pd.DataFrame({'imagename':imagename,\n",
    "                             'label':imageset,\n",
    "                             'BBoxes': json.dumps(BB),\n",
    "                            'imagepath':img_path,\n",
    "                            'xshape':x.shape[0],\n",
    "                            'yshape':x.shape[1]},index=[0])\n",
    "            dftrain=pd.concat([dftrain,df])\n",
    "\n",
    "\n",
    "    dftrain.index=range(len(dftrain))\n",
    "    dftrain_cropped.index=range(len(dftrain_cropped))\n",
    "\n",
    "    dftrain.to_hdf('Meta.h5','train')\n",
    "    dftrain_cropped.to_hdf('Meta.h5','train_cropped')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain=pd.read_hdf('Meta.h5','train')\n",
    "dftrain_cropped=pd.read_hdf('Meta.h5','train_cropped')\n",
    "\n",
    "dftrain.index=range(len(dftrain))\n",
    "dftrain_cropped.index=range(len(dftrain_cropped))\n",
    "# dftrain['BBoxes']=dftrain['BBoxes'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "                    rotation_range=40,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "BBrunData='BBrunData.npz'\n",
    "if not os.path.isfile(BBrunData):\n",
    "    \n",
    "    Xtrain_resized=None\n",
    "    Y=None\n",
    "\n",
    "    mean_R = []\n",
    "    mean_G = []\n",
    "    mean_B = []\n",
    "\n",
    "    #loading images\n",
    "    for ind in dftrain.index:\n",
    "        print ind, len(dftrain),'\\r',\n",
    "\n",
    "        img = image.load_img(dftrain.loc[ind,'imagepath'],target_size=(224, 224,3))\n",
    "        x = image.img_to_array(img)\n",
    "\n",
    "        #finding the mean image\n",
    "        a = np.mean(x[:,:,0])\n",
    "        mean_R = np.append(mean_R,a)\n",
    "        b = np.mean(x[:,:,1])\n",
    "        mean_G = np.append(mean_G,b)\n",
    "        c = np.mean(x[:,:,2])\n",
    "        mean_B = np.append(mean_B,c)\n",
    "\n",
    "\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        if Xtrain_resized is None:\n",
    "            Xtrain_resized=x\n",
    "            Y=np.array([ dftrain.loc[ind,'label'] ])\n",
    "        else:\n",
    "            Xtrain_resized=np.vstack((Xtrain_resized,x) )\n",
    "            Y=np.vstack((Y,np.array([ dftrain.loc[ind,'label'] ])) )\n",
    "            \n",
    "    print \"Getting augmented data\"\n",
    "    \n",
    "    \n",
    "    y=Y.reshape(1,-1)[0]\n",
    "    classsizes=[]\n",
    "    labels=np.unique(y)\n",
    "    for cls in labels:\n",
    "        classsizes.append(len(y[y==cls]))\n",
    "\n",
    "    classsizes=np.array(classsizes)\n",
    "    print classsizes\n",
    "    mxsz=1.3*max(classsizes)\n",
    "    meansz=1.3*np.mean(classsizes)\n",
    "    \n",
    "    Xaug=None\n",
    "    Yaug=None\n",
    "    for clsind in range(len(classsizes)):\n",
    "        print labels[clsind],\" original size \",len(Y[y==labels[clsind]])\n",
    "        iters= int( (mxsz-classsizes[clsind])/20 ) /2\n",
    "        for batch in datagen.flow(Xtrain_resized[y==labels[clsind],:,:,:],Y[y==labels[clsind]], batch_size=20):\n",
    "            if Xaug is None:\n",
    "                Xaug=batch[0]\n",
    "                Yaug=batch[1]\n",
    "            else:\n",
    "                Xaug=np.vstack((Xaug,batch[0]) )\n",
    "                Yaug=np.vstack((Yaug, batch[1] ) )\n",
    "\n",
    "            iters=iters-1\n",
    "            print labels[clsind],clsind,classsizes[clsind],mxsz,iters,'\\r',\n",
    "            if iters<0:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    #Mean Image\n",
    "    I_R = np.mean(mean_R)\n",
    "    I_G = np.mean(mean_G)\n",
    "    I_B = np.mean(mean_B)\n",
    "    print (I_R,I_G,I_B)\n",
    "    \n",
    "    Labels=np.unique(Y)\n",
    "    Ylabels=Y\n",
    "    for ind,l in enumerate(Labels):\n",
    "        Y[np.argwhere(Y==l)]=ind\n",
    "    Y=Y.astype(int)\n",
    "    \n",
    "    Yauglabels=Yaug\n",
    "    for ind,l in enumerate(Labels):\n",
    "        Yaug[np.argwhere(Yaug==l)]=ind\n",
    "    Yaug=Yaug.astype(int)\n",
    "    \n",
    "    \n",
    "    Ybinary=np_utils.to_categorical(Y)\n",
    "    Ybinary=Ybinary.astype(int)\n",
    "\n",
    "    np.savez(BBrunData,I_R=I_R,I_G=I_G,I_B=I_B,\n",
    "             Xtrain_resized=Xtrain_resized,\n",
    "             Labels=Labels,Ylabels=Ylabels,Ybinary=Ybinary,Y=Y,Yaug=Yaug,Xaug=Xaug,Yauglabels=Yauglabels)\n",
    "else:\n",
    "    print \"loading saved data\"\n",
    "    data=np.load(BBrunData)\n",
    "    I_R=data['I_R']\n",
    "    I_G=data['I_G']\n",
    "    I_B=data['I_B']\n",
    "    Xtrain_resized=data['Xtrain_resized']\n",
    "    Labels=data['Labels']\n",
    "    Ylabels=data['Ylabels']\n",
    "    Ybinary=data['Ybinary']\n",
    "    Y=data['Y']\n",
    "    \n",
    "    Xaug=data['Xaug']\n",
    "    Yaug=data['Yaug']\n",
    "    \n",
    "    print (I_R,I_G,I_B)\n",
    "    \n",
    "# -----------------   Cropped --------------------\n",
    "\n",
    "BBrunData_cropped='BBrunData_cropped.npz'\n",
    "if not os.path.isfile(BBrunData_cropped):\n",
    "    \n",
    "    Xtrain_cropped_resized=None\n",
    "    Y_cropped=None\n",
    "\n",
    "    #loading images\n",
    "    for ind in dftrain_cropped.index:\n",
    "        print ind, len(dftrain_cropped),'\\r',\n",
    "\n",
    "        img = image.load_img(dftrain_cropped.loc[ind,'imagepath'],target_size=(224, 224,3))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        if Xtrain_cropped_resized is None:\n",
    "            Xtrain_cropped_resized=x\n",
    "            Y_cropped=np.array([ dftrain_cropped.loc[ind,'label'] ])\n",
    "        else:\n",
    "            Xtrain_cropped_resized=np.vstack((Xtrain_cropped_resized,x) )\n",
    "            Y_cropped=np.vstack((Y_cropped,np.array([ dftrain_cropped.loc[ind,'label'] ])) )\n",
    "            \n",
    "    print \"Getting augmented data\"\n",
    "    \n",
    "    \n",
    "    y=Y_cropped.reshape(1,-1)[0]\n",
    "    classsizes=[]\n",
    "#     labels=np.unique(y)\n",
    "    labels=Labels\n",
    "    for cls in Labels:\n",
    "        classsizes.append(len(y[y==cls]))\n",
    "\n",
    "    classsizes=np.array(classsizes)\n",
    "    print classsizes\n",
    "    mxsz=1.3*max(classsizes)\n",
    "    meansz=1.3*np.mean(classsizes)\n",
    "    \n",
    "    Xaug_cropped=None\n",
    "    Yaug_cropped=None\n",
    "    for clsind in range(len(classsizes)):\n",
    "        print labels[clsind],\" original size \",len(Y_cropped[y==labels[clsind]])\n",
    "        iters= int( (mxsz-classsizes[clsind])/20 ) /2\n",
    "        for batch in datagen.flow(Xtrain_cropped_resized[y==labels[clsind],:,:,:],Y_cropped[y==labels[clsind]], batch_size=20):\n",
    "            if Xaug_cropped is None:\n",
    "                Xaug_cropped=batch[0]\n",
    "                Yaug_cropped=batch[1]\n",
    "            else:\n",
    "                Xaug_cropped=np.vstack((Xaug_cropped,batch[0]) )\n",
    "                Yaug_cropped=np.vstack((Yaug_cropped, batch[1] ) )\n",
    "\n",
    "            iters=iters-1\n",
    "            print labels[clsind],clsind,classsizes[clsind],mxsz,iters,'\\r',\n",
    "            if iters<0:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    Ylabels_cropped=Y_cropped\n",
    "    for ind,l in enumerate(Labels):\n",
    "        Y_cropped[np.argwhere(Y_cropped==l)]=ind\n",
    "        \n",
    "    Yauglabels_cropped=Yaug_cropped\n",
    "    for ind,l in enumerate(Labels):\n",
    "        Yaug_cropped[np.argwhere(Yaug_cropped==l)]=ind\n",
    "    Yaug_cropped=Yaug_cropped.astype(int)\n",
    "    \n",
    "    Ybinary_cropped=np_utils.to_categorical(Y_cropped)\n",
    "    Y_cropped=Y_cropped.astype(int)\n",
    "    Ybinary_cropped=Ybinary_cropped.astype(int)\n",
    "\n",
    "    np.savez(BBrunData_cropped,Xtrain_cropped_resized=Xtrain_cropped_resized,\n",
    "             Ylabels_cropped=Ylabels_cropped,Ybinary_cropped=Ybinary_cropped,Y_cropped=Y_cropped,\n",
    "             Yaug_cropped=Yaug_cropped,Xaug_cropped=Xaug_cropped,Yauglabels_cropped=Yauglabels_cropped)\n",
    "else:\n",
    "    print \"loading saved _cropped data\"\n",
    "    data=np.load(BBrunData_cropped)\n",
    "    Xtrain_cropped_resized=data['Xtrain_cropped_resized']\n",
    "    Ylabels_cropped=data['Ylabels_cropped']\n",
    "    Y_cropped=data['Y_cropped']\n",
    "    \n",
    "    Xaug_cropped=data['Xaug_cropped']\n",
    "    Yaug_cropped=data['Yaug_cropped']\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getscaledBB(ind):\n",
    "    ppbox=json.loads( dftrain.loc[ind,'BBoxes']  )\n",
    "    if len(ppbox)==0:\n",
    "        return None\n",
    "    \n",
    "#     ppbox=copy.deepcopy(ppbox)\n",
    "#     x = image.img_to_array(img)\n",
    "    xmax=dftrain.loc[ind,'yshape']\n",
    "    ymax=dftrain.loc[ind,'xshape']\n",
    "    \n",
    "    leftcorner=0\n",
    "    rightcorner=1\n",
    "    for i in range(len(ppbox)):\n",
    "        ppbox[i][leftcorner][0]=int( 224*ppbox[i][leftcorner][0]/xmax   ) \n",
    "        ppbox[i][rightcorner][0]=int( 224*ppbox[i][rightcorner][0]/xmax)\n",
    "\n",
    "        ppbox[i][leftcorner][1]=int( 224*ppbox[i][leftcorner][1]/ymax    )\n",
    "        ppbox[i][rightcorner][1]=int( 224*ppbox[i][rightcorner][1]/ymax )\n",
    "    \n",
    "    return ppbox\n",
    "\n",
    "def pltimagbbox(img,ppbox,rects=[]):\n",
    "    ppbox=copy.deepcopy(ppbox)\n",
    "    draw=ImageDraw.Draw(img)\n",
    "    for rect in rects:\n",
    "        draw.rectangle(rect,outline='#ff0000')\n",
    "        rect[0]=np.array(rect[0])+1\n",
    "        rect[1]=np.array(rect[1])-1\n",
    "        draw.rectangle([tuple(rect[0]),tuple(rect[1])],outline='#ff0000')\n",
    "        rect[0]=np.array(rect[0])+1\n",
    "        rect[1]=np.array(rect[1])-1\n",
    "        draw.rectangle([tuple(rect[0]),tuple(rect[1])],outline='#ff0000')\n",
    "\n",
    "    if ppbox is not None:\n",
    "        for trbox in ppbox:\n",
    "            draw.rectangle([tuple(trbox[0]),tuple(trbox[1])],outline='#00ff00')\n",
    "            trbox[0]=np.array(trbox[0])+1\n",
    "            trbox[1]=np.array(trbox[1])-1\n",
    "            draw.rectangle([tuple(trbox[0]),tuple(trbox[1])],outline='#00ff00')\n",
    "            trbox[0]=np.array(trbox[0])+1\n",
    "            trbox[1]=np.array(trbox[1])-1\n",
    "            draw.rectangle([tuple(trbox[0]),tuple(trbox[1])],outline='#00ff00')\n",
    "    \n",
    "    del draw\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def CheckOverlap(x0tr,y0tr,x1tr,y1tr,x0,y0,x1,y1):\n",
    "    # return True if overlapp is good\n",
    "    # first make sure x0 is min and y0 is min\n",
    "    if x0tr>x1tr:\n",
    "        x0tr,x1tr=x1tr,x0tr\n",
    "    if y0tr>y1tr:\n",
    "        y0tr,y1tr=y1tr,y0tr\n",
    "    if x0>x1:\n",
    "        x0,x1=x1,x0\n",
    "    if y0>y1:\n",
    "        y0,y1=y1,y0\n",
    "    \n",
    "#     print \"check overlap\"\n",
    "    \n",
    "    Atr=((x1tr-x0tr)*(y1tr-y0tr))\n",
    "    A=((x1-x0)*(y1-y0))\n",
    "    \n",
    "    xl=max(x0tr,x0)\n",
    "    yl=max(y0tr,y0)\n",
    "    \n",
    "    xr=min(x1tr,x1)\n",
    "    yr=min(y1tr,y1)\n",
    "    \n",
    "    interarea=max((xr-xl),0)*max((yr-yl),0)\n",
    "\n",
    "    \n",
    "    if interarea/Atr >=0.5 and min(A,Atr)/max(A,Atr)>=0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    # area of interesection with truth is greater than 70%\n",
    "\n",
    "def RemoveMean(X):\n",
    "    print \"remocing mean  \",I_R,I_G,I_B\n",
    "    if len(X.shape)==4:\n",
    "        print \"batch remove\"\n",
    "        for ind in range(X.shape[0]):\n",
    "            print ind,X.shape[0],'\\r',\n",
    "            X[ind,:,:,0] = X[ind,:,:,0] - I_R\n",
    "            X[ind,:,:,1] = X[ind,:,:,1] - I_G\n",
    "            X[ind,:,:,2] = X[ind,:,:,2] - I_B\n",
    "    else:\n",
    "        print \"single remove\"\n",
    "        X[:,:,0] = X[:,:,0] - I_R\n",
    "        X[:,:,1] = X[:,:,1] - I_G\n",
    "        X[:,:,2] = X[:,:,2] - I_B\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=1791\n",
    "print dftrain.loc[ind,'imagepath']\n",
    "print \"UNscaled image\"\n",
    "img0=image.load_img(dftrain.loc[ind,'imagepath'])\n",
    "ppbox=json.loads( dftrain.loc[ind,'BBoxes'] )\n",
    "pltimagbbox(img0,ppbox,rects=[[(33,33),(50,50)]] )\n",
    "\n",
    "print \"scaled image\"\n",
    "\n",
    "x=np.copy( Xtrain_resized[ind,:,:,:] )\n",
    "y=RemoveMean(np.copy(x))\n",
    "print np.array_equal(x,y)\n",
    "img=image.array_to_img(x)\n",
    "ppbox=getscaledBB(ind)\n",
    "print \"ppbox = \",ppbox\n",
    "pltimagbbox(img,ppbox,rects=[[(33,33),(50,50)]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrainCLF(itr,Xall,Yall):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xall, Yall, test_size=0.33, random_state=itr)\n",
    "    \n",
    "    print \"# Tuning hyper-parameters for %s\" % score\n",
    "    print \"\"\n",
    "    clf = GridSearchCV(LinearSVC(C=1), {'C': [1, 10, 100, 1000,10000]}, cv=5,scoring='%s_macro' % 'precision',n_jobs=5)\n",
    "\n",
    "#     clf = GridSearchCV(SVC(C=1.0, kernel='linear',max_iter=1e5,probability=True,shrinking=True), {'C': [1, 10, 100, 1000,10000]}, cv=5,\n",
    "#                        scoring='%s_macro' % 'precision',n_jobs=5)\n",
    "\n",
    "\n",
    "    clf.fit(X_train,  y_train.reshape(1,-1)[0])\n",
    "\n",
    "    print \"Best parameters set found on development set:\"\n",
    "    print \n",
    "    print clf.best_params_\n",
    "    print \n",
    "    print \"Grid scores on development set:\"\n",
    "    print\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print \"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params) \n",
    "    print\n",
    "\n",
    "    print \"Detailed classification report:\"\n",
    "    print\n",
    "    print \"The model is trained on the full development set.\"\n",
    "    print \"The scores are computed on the full evaluation set.\"\n",
    "    print\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print classification_report(y_true, y_pred)\n",
    "\n",
    "    dumpname= 'Resnet50'+'_'+'SVC'+'_'+'precision'+str(itr)+'.pkl'\n",
    "    joblib.dump(clf, os.path.join('models_cropped', dumpname) )\n",
    "    \n",
    "    return clf                                                                                          \n",
    "\n",
    "\n",
    "    \n",
    "def doprediction(XcropFP,YcropFP,clf,model,append2Xall=True):\n",
    "    kkk=0\n",
    "    perclassdonelist={}\n",
    "    for xind in dftrain.index:\n",
    "        print \"-----\"*20\n",
    "        print xind,dftrain.loc[xind,'imagepath']\n",
    "        print xind,' of ',len(dftrain)\n",
    "        x= np.copy(Xtrain_resized[xind,:,:,:]) \n",
    "        \n",
    "        ppbox=getscaledBB(xind)\n",
    "        if ppbox is None:\n",
    "            print \"skipping image as no ppbox\"\n",
    "            continue\n",
    "            \n",
    "        if dftrain.loc[xind,'label'] not in perclassdonelist:\n",
    "            perclassdonelist[dftrain.loc[xind,'label']]=0\n",
    "        else:\n",
    "            perclassdonelist[dftrain.loc[xind,'label']]=perclassdonelist[dftrain.loc[xind,'label']]+1\n",
    "        \n",
    "        if perclassdonelist[dftrain.loc[xind,'label']]>25:\n",
    "            continue\n",
    "            \n",
    "        img=image.array_to_img(x)\n",
    "        pltimagbbox(img,ppbox,rects=[] )\n",
    "        \n",
    "        XX=None\n",
    "        YY=None\n",
    "        for h in range(50,90,20):\n",
    "            for w in range(50,90,20):\n",
    "                for j in range(0,224,20):\n",
    "                    for i in range(0,224,20):\n",
    "                        x0=i\n",
    "                        y0=j\n",
    "                        x1=x0+w\n",
    "                        y1=y0+h\n",
    "\n",
    "                        print h,w,j,i,'\\r',\n",
    "                        if x1>=224 or y1>=224:\n",
    "                            break\n",
    "\n",
    "                        img=image.array_to_img(x)\n",
    "                        immg=image.array_to_img(x[y0:y1,x0:x1,:]).resize((224, 224))\n",
    "                        y = image.img_to_array(immg)\n",
    "                        y[:,:,0] = y[:,:,0] - I_R\n",
    "                        y[:,:,1] = y[:,:,1] - I_G\n",
    "                        y[:,:,2] = y[:,:,2] - I_B\n",
    "                        y = np.expand_dims(y, axis=0)\n",
    "                        yext=model.predict(y)\n",
    "                        \n",
    "                        pp=clf.predict(yext)[0]\n",
    "                        \n",
    "                        if pp == Y[xind] :\n",
    "#                             print \"predicted label correctly\",pp\n",
    "                            overlap=False\n",
    "                            if ppbox is not None: \n",
    "#                                 print ppbox\n",
    "                                for trbox in ppbox:\n",
    "                                    overlap=overlap or CheckOverlap(trbox[0][0],trbox[0][1],trbox[1][0],trbox[1][1],x0,y0,x1,y1)\n",
    "#                                 print \"overlap \",overlap\n",
    "#                                 pltimagbbox(img,ppbox,rects=[[(x0,y0),(x1,y1)]] )\n",
    "                            else:\n",
    "                                print \"No true box avaibles for this image\"\n",
    "#                                 pltimagbbox(img,ppbox,rects=[[(x0,y0),(x1,y1)]] )\n",
    "\n",
    "                            if overlap==False:\n",
    "                                print \"appending FP case to training set \",kkk,'\\r',\n",
    "                                kkk=kkk+1\n",
    "                                if XX is None:\n",
    "                                    XX=yext\n",
    "                                    YY=np.array([4])\n",
    "                                else:\n",
    "                                    XX=np.vstack((XX,yext))\n",
    "                                    YY=np.vstack((YY,np.array([4]) ))\n",
    "        \n",
    "                            else:\n",
    "                                pltimagbbox(img,ppbox,rects=[[(x0,y0),(x1,y1)]] )\n",
    "        try:\n",
    "            if XX.shape[0]>25:\n",
    "                rt=np.random.choice( XX.shape[0], 25)\n",
    "                XX=XX[rt,:]\n",
    "                YY=YY[rt]\n",
    "                \n",
    "            if XcropFP is None:\n",
    "                XcropFP=XX\n",
    "                YcropFP=YY\n",
    "            else:\n",
    "                XcropFP=np.vstack((XcropFP,XX))\n",
    "                YcropFP=np.vstack((YcropFP,YY))\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "        \n",
    "        np.savez('NewFPcropped_'+str(kkk)+'.npz',XcropFP=XcropFP,YcropFP=YcropFP)\n",
    "        \n",
    "    return (XcropFP,YcropFP)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NewFPcropped_3.npz'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'NewFPcropped_'+str(3)+'.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading initial mean extracted data\n",
    "data=np.load('ExtractedFeatures_cropped.npz')\n",
    "Xtrain_cropped_ext=data['Xext'][()]['ResNet50']\n",
    "\n",
    "\n",
    "clf=joblib.load(os.path.join('models_cropped','ResNet50_LinearSVC_recall.pkl') )\n",
    "model=ResNet50(weights='imagenet')\n",
    "\n",
    "print \"OK\"\n",
    "XcropFP=None\n",
    "YcropFP=None\n",
    "XcropFP,YcropFP=doprediction(XcropFP,YcropFP,clf,model,append2Xall=True)\n",
    "np.savez('NewFPcropped.npz',XcropFP=XcropFP,YcropFP=YcropFP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Y[0]==np.array([[7]]):\n",
    "    print \"ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf=joblib.load(os.path.join('models_cropped','ResNet50_LinearSVC_recall.pkl') )\n",
    "model=ResNet50(weights='imagenet')\n",
    "\n",
    "if hasattr(clf,'predict_proba')==False:\n",
    "    def predict_proba(self,X):\n",
    "        y=np.zeros((1,clf.best_estimator_.coef_.shape[0]))\n",
    "        ind=clf.predict(X)[0]\n",
    "        y[0][ind]=1\n",
    "        return y\n",
    "    clf.predict_proba=types.MethodType( predict_proba, clf )\n",
    "\n",
    "fish='LAG' \n",
    "\n",
    "imageind=40\n",
    "\n",
    "img_path=Allimages[fish][imageind]\n",
    "print img_path\n",
    "img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "height=50\n",
    "width=70\n",
    "\n",
    "# height=45\n",
    "# width=80\n",
    "L=[]\n",
    "for h in range(50,80,10):\n",
    "    for w in range(50,80,10):\n",
    "\n",
    "        for j in range(0,224,10):\n",
    "            for i in range(0,224,10):\n",
    "                x0=i\n",
    "                y0=j\n",
    "                x1=x0+w\n",
    "                y1=y0+h\n",
    "\n",
    "                print h,w,j,i,'\\r',\n",
    "                if x1>=224 or y1>=224:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                # plt.show()\n",
    "\n",
    "\n",
    "#                 img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "                img = image.load_img(img_path)\n",
    "                img,imgresized,imgresizedbbox=CustomRescaleBBimgs(img,All_bboxes[fish][imageind],Allimages_shapes[fish][imageind])\n",
    "                ppbox=getscaledBB(All_bboxes[fish][imageind],Allimages_shapes[fish][imageind])\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                x = image.img_to_array(imgresized)\n",
    "                immg=image.array_to_img(x[y0:y1,x0:x1,:])\n",
    "                immg=immg.resize((224, 224))\n",
    "                img2 = immg.rotate(45).resize((224, 224))\n",
    "\n",
    "                y = image.img_to_array(immg)\n",
    "                y[:,:,0] = y[:,:,0] - I_R\n",
    "                y[:,:,1] = y[:,:,1] - I_G\n",
    "                y[:,:,2] = y[:,:,2] - I_B\n",
    "                \n",
    "                y = np.expand_dims(y, axis=0)\n",
    "                yext=model.predict(y)\n",
    "                pp=clf.predict_proba(yext)[0]\n",
    "#                 print pp\n",
    "                dp=pd.DataFrame({'probs':pp,'labels':list(DFEC.Labels)})\n",
    "                dp.sort_values(by='probs',ascending=False,inplace=True)\n",
    "                dp.index=range(len(dp))\n",
    "                pp=list(pp)\n",
    "                pp.append(w)\n",
    "                pp.append(h)\n",
    "                pp.append(x0)\n",
    "                pp.append(y0)\n",
    "                pp.append(x1)\n",
    "                pp.append(y1)\n",
    "                L.append(pp)\n",
    "\n",
    "                if fish == dp.loc[0,'labels'].tolist() :\n",
    "                    print dp.loc[0:2,:]\n",
    "                    print x0,y0,x1,y1\n",
    "\n",
    "                    fig,ax=plt.subplots(1,2)\n",
    "                    draw=ImageDraw.Draw(imgresized)\n",
    "                    draw.rectangle([(x0,y0),(x1,y1)],outline='#ff0000')\n",
    "                    for trbox in ppbox:\n",
    "                        print \"overalap \",CheckOverlap(trbox[0][0],trbox[0][1],trbox[1][0],trbox[1][1],x0,y0,x1,y1)\n",
    "                        draw.rectangle([tuple(trbox[0]),tuple(trbox[1])],outline='#00ff00')\n",
    "                    del draw\n",
    "                    ax[0].imshow(imgresized)\n",
    "\n",
    "                    \n",
    "                    ax[1].imshow(immg)\n",
    "\n",
    "            #         fig.suptitle(str( clf.predict_proba(yext)) )\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "\n",
    "                    print \"-\"*100\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "dfhist=pd.DataFrame(L,columns=list(DFEC.Labels)+['w','h','x0','y0','x1','y1'])\n",
    "# fig,ax=plt.subplots(1,1,figsize=(15,10))\n",
    "# dfhist.hist(stacked=True, bins=np.linspace(0,1,20),ax=ax,normed=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('TrainImageMean_cropped.npz')\n",
    "data['I_R'],data['I_G'],data['I_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(array(96.40368659262118), array(107.12191881841852), array(99.89469193894617))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fish='LAG' \n",
    "\n",
    "imageind=41\n",
    "\n",
    "img_path=Allimages[fish][imageind]\n",
    "print img_path\n",
    "img = image.load_img(img_path)\n",
    "\n",
    "img,imgresized,imgresizedbbox=CustomRescaleBB(img,All_bboxes[fish][imageind],Allimages_shapes[fish][imageind])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(imgresized)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(imgresizedbbox)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Allimages_shapes[fish][imageind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp=pd.DataFrame(L,columns=list(DFEC.Labels)+['w','h','x0','y0'])\n",
    "dp['x1']=dp['x0']+dp['w']\n",
    "dp['y1']=dp['y0']+dp['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S=[]\n",
    "for x in range(80,80+50,2):\n",
    "    for y in range(90,90+45,2):\n",
    "        print x,y,'\\r',\n",
    "#         ds=dp[(dp['x0']<=x) & (x<=dp['x1']) & (dp['y0']<=y) & (y<=dp['y1'])][list(DFEC.Labels)]\n",
    "#         k=[]\n",
    "#         for cc in DFEC.Labels:\n",
    "#             k.append( ds[ds[cc]>=0.3][cc].mean() )\n",
    "#         S.append( [x,y,DFEC.Labels[np.argmax( np.array(k) )]]  )\n",
    "        fig,ax=plt.subplots(1,1,figsize=(10,7))\n",
    "        dfhist.hist(stacked=True, bins=np.linspace(0,1,20),ax=ax,normed=True)\n",
    "        plt.show()\n",
    "\n",
    "# ds=pd.DataFrame(S,columns=['x','y','label'])\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1)\n",
    "x0=80\n",
    "y0=90\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "draw=ImageDraw.Draw(img)\n",
    "draw.rectangle([(x0,y0),(x0+50,y0+45)],outline='#ff0000')\n",
    "del draw\n",
    "ax.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "fish='SHARK'\n",
    "dg=ds[ds['label']==fish][['x','y']]\n",
    "x0,y0=ds[ds['label']==fish][['x','y']].min().tolist()\n",
    "x1,y1=ds[ds['label']==fish][['x','y']].max().tolist()\n",
    "print x0,y0\n",
    "print x1,y1\n",
    "fig,ax=plt.subplots(1,1)\n",
    "for ind in dg.index:\n",
    "    draw=ImageDraw.Draw(img)\n",
    "    x0=dg.loc[ind,'x']\n",
    "    y0=dg.loc[ind,'y']\n",
    "    draw.rectangle([(x0,y0),(x0+2,y0+2)],outline='#ff0000')\n",
    "    del draw\n",
    "ax.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "img = cv2.imread(img_path,1)\n",
    "img = cv2.resize(img, (224, 224), cv2.INTER_LINEAR).astype(np.uint8)\n",
    "# img = img.transpose((1,0,2))\n",
    "# im = np.expand_dims(im, axis=0)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print type(img),img.shape\n",
    "plt.imshow(img[:100,:100,:])\n",
    "# plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "# //w=187,h=76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=Y.reshape(1,-1)[0]\n",
    "Xmean[y==0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load('MeanRemovedData_cropped.npz')\n",
    "Xmean=data['Xmean']\n",
    "Y=data['Y']\n",
    "Y=Y.astype(int)\n",
    "\n",
    "Labels=data['Labels']\n",
    "Ybinary=data['Ybinary']\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=90,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "y=Y.reshape(1,-1)[0]\n",
    "classsizes=[]\n",
    "for i in range(0,len(np.unique(y))):\n",
    "    classsizes.append(len(y[y==i]))\n",
    "\n",
    "classsizes=np.array(classsizes)\n",
    "mxsz=1.3*max(classsizes)\n",
    "meansz=1.3*np.mean(classsizes)\n",
    "for clsind in range(len(classsizes)):\n",
    "    print \n",
    "    iters= int( (mxsz-classsizes[clsind])/20 ) \n",
    "    for batch in datagen.flow(Xmean[y==clsind,:,:,:],Y[y==clsind], batch_size=20):\n",
    "        Xmean=np.vstack((Xmean,batch[0]) )\n",
    "        Y=np.vstack((Y, batch[1] ) )\n",
    "        \n",
    "        iters=iters-1\n",
    "        print clsind,classsizes[clsind],mxsz,iters,'\\r',\n",
    "        if iters==0:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xmean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=DFEC.Y.reshape(1,-1)[0]\n",
    "classsizes=[]\n",
    "labels=np.unique(y)\n",
    "for cls in labels:\n",
    "    classsizes.append(len(y[y==cls]))\n",
    "print DFEC.Labels\n",
    "classsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
